{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egtdWTvcOeTy",
    "outputId": "280c1e96-d7c9-4c5f-ba0f-3b9035caacfe",
    "ExecuteTime": {
     "end_time": "2024-12-17T21:19:44.306904Z",
     "start_time": "2024-12-17T21:19:44.289307Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_-uxEt3O1Y_",
    "outputId": "46a862d9-f58d-4bba-9a96-7e7149cae048",
    "ExecuteTime": {
     "end_time": "2024-12-17T19:07:37.104364Z",
     "start_time": "2024-12-17T19:07:01.528126Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large-mnli', do_lower_case=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szymong/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/szymong/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szymong/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abd86f8c9bd44dba965b957fdd399b08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1a562e4e3504d0d8bc980efa9a72b2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ee9c83f4dcb4681aeae2138236a52eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2981704482d4ce185b0560ca93c68aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d064634e07c5495e9325226c2812e81d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the BERT tokenizer.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoading BERT tokenizer...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mroberta-large-mnli\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_lower_case\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py:786\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    784\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[1;32m    785\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    788\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1983\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1981\u001B[0m             resolved_vocab_files[file_id] \u001B[38;5;241m=\u001B[39m download_url(file_path, proxies\u001B[38;5;241m=\u001B[39mproxies)\n\u001B[1;32m   1982\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1983\u001B[0m         resolved_vocab_files[file_id] \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1984\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1985\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1986\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1987\u001B[0m \u001B[43m            \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1988\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1989\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1990\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1991\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1992\u001B[0m \u001B[43m            \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1993\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1994\u001B[0m \u001B[43m            \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1995\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1996\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1997\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1998\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1999\u001B[0m         commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unresolved_files) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py:430\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    427\u001B[0m user_agent \u001B[38;5;241m=\u001B[39m http_user_agent(user_agent)\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 430\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to request access at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and pass a token having permission to this repo either \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    449\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:860\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m    840\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[1;32m    841\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[1;32m    842\u001B[0m         local_dir\u001B[38;5;241m=\u001B[39mlocal_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    857\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    858\u001B[0m     )\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 860\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[1;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[1;32m    864\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[1;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[1;32m    875\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:923\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m    919\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m pointer_path\n\u001B[1;32m    921\u001B[0m \u001B[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001B[39;00m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;66;03m# If we can't, a HEAD request error is returned.\u001B[39;00m\n\u001B[0;32m--> 923\u001B[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001B[38;5;241m=\u001B[39m \u001B[43m_get_metadata_or_catch_error\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m    \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    932\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    933\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    934\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    935\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrelative_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    936\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    938\u001B[0m \u001B[38;5;66;03m# etag can be None for several reasons:\u001B[39;00m\n\u001B[1;32m    939\u001B[0m \u001B[38;5;66;03m# 1. we passed local_files_only.\u001B[39;00m\n\u001B[1;32m    940\u001B[0m \u001B[38;5;66;03m# 2. we don't have a connection\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001B[39;00m\n\u001B[1;32m    947\u001B[0m \u001B[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head_call_error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m     \u001B[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1374\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1373\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1374\u001B[0m         metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1375\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1377\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n\u001B[1;32m   1378\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m storage_folder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m relative_filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1379\u001B[0m             \u001B[38;5;66;03m# Cache the non-existence of the file\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1294\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1291\u001B[0m hf_headers[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccept-Encoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midentity\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001B[39;00m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1294\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1296\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1298\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1302\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m hf_raise_for_status(r)\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;66;03m# Return\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:278\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;66;03m# Recursively follow relative redirects\u001B[39;00m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 278\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m300\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m399\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:301\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m    300\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[0;32m--> 301\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mget_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m hf_raise_for_status(response)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:746\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m--> 746\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/models.py:902\u001B[0m, in \u001B[0;36mResponse.content\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 902\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:1060\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1059\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1060\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m   1063\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:949\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    946\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[0;32m--> 949\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    951\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[1;32m    953\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:902\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    894\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m IncompleteRead(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_bytes_read, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength_remaining)\n\u001B[1;32m    895\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m read1 \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m    896\u001B[0m         (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength_remaining \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[1;32m    897\u001B[0m     ):\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001B[39;00m\n\u001B[1;32m    901\u001B[0m         \u001B[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001B[39;00m\n\u001B[0;32m--> 902\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    905\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_bytes_read \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py:124\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[0;34m(self, type, value, traceback)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 124\u001B[0m         \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:748\u001B[0m, in \u001B[0;36mHTTPResponse._error_catcher\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 748\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m SocketTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    751\u001B[0m         \u001B[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001B[39;00m\n\u001B[1;32m    752\u001B[0m         \u001B[38;5;66;03m# there is yet no clean way to get at it from this context.\u001B[39;00m\n\u001B[1;32m    753\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ReadTimeoutError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRead timed out.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwlWUAIgAKGn",
    "outputId": "6f0150d8-fa97-4ed0-ec48-4103a917c39b",
    "ExecuteTime": {
     "end_time": "2024-12-17T19:09:28.943936Z",
     "start_time": "2024-12-17T19:09:28.872203Z"
    }
   },
   "source": [
    "import json\n",
    "with open(\"train_claims_quantemp.json\") as f:\n",
    "  train_data = json.load(f)\n",
    "len(train_data\n",
    "    )"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_claims_quantemp.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain_claims_quantemp.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      3\u001B[0m   train_data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mlen\u001B[39m(train_data\n\u001B[1;32m      5\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[0;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train_claims_quantemp.json'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjbPPEMKW4CM",
    "outputId": "7e60411d-5bf1-4e27-f0bc-19b8280c1a92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [{'link': 'http://tsn.ua/politika/pid-zavalami-zgoriloyi-bazi-berkutu-u-lvovi-znayshli-dvoh-mertvih-silovikiv-335713.html',\n",
       "   'description': 'occured'},\n",
       "  {'link': 'http://24tv.ua/home/showSingleNews.do?lvovskiy_berkut_stal_na_koleni_pered_lvovyanami_video&objectId=413464&lang=ru',\n",
       "   'description': 'kneeled'}],\n",
       " 'country_of_origin': 'ukraine',\n",
       " 'label': 'False',\n",
       " 'published': '2014-03-26',\n",
       " 'url': 'https://www.stopfake.org/en/fake-commandos-from-berkut-who-refused-to-kneel-have-been-burned-alive-in-lviv/',\n",
       " 'crawled_date': '2014-03-26T10:38:09',\n",
       " 'lang': 'en',\n",
       " 'fact_source': 'stopfake',\n",
       " 'supported': True,\n",
       " 'label_original': 'FAKE',\n",
       " 'claim': 'FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv',\n",
       " 'doc': 'The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.',\n",
       " 'complexity_class': 'complex',\n",
       " 'numerical': True,\n",
       " 'taxonomy_label': 'statistical'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea6PlA4bW5UO",
    "outputId": "aa084206-c1b5-4799-8952-3e8cb84f34ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"val_claims_quantemp.json\") as f:\n",
    "  val_data = json.load(f)\n",
    "len(val_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZT-q3h48kxdg",
    "outputId": "d5290e0b-29f5-4839-c1b2-065411713679"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stated_in': 'stated on October 5, 2021 in a tweet',\n",
       " 'date_stated': '2021-10-05',\n",
       " 'sources': [{'link': 'https://twitter.com/RepFeenstra/status/1445480085243367427?s=20',\n",
       "   'description': 'Twitter, Rep. Randy Feenstra, Oct. 5, 2021'},\n",
       "  {'link': 'https://twitter.com/JenniferJJacobs/status/1445477075947646978?s=20',\n",
       "   'description': 'Twitter, Jennifer Jacobs, Oct. 5, 2021'},\n",
       "  {'link': 'https://t.co/Qk1Chwurc3?amp=1',\n",
       "   'description': 'U.S. Energy Information Administration, Annual Energy Outlook 2021, 2021'},\n",
       "  {'link': 'https://www.desmoinesregister.com/story/money/agriculture/2020/09/14/epa-says-reject-requests-cut-ethanol-use/5790852002/',\n",
       "   'description': 'The Des Moines Register, \"With rural voters’ support at risk, Trump administration backs most ethanol mandates,\" by Donnelle Eller, Sept. 14, 2020'},\n",
       "  {'link': 'https://joebiden.com/2020/09/15/statement-by-vice-president-joe-biden-on-need-to-stand-with-farmers-and-biofuel-producers-after-donald-trumps-latest-insult-to-ethanol-industry/',\n",
       "   'description': 'Biden-Harris campaign, \"Statement by VP Biden on standing with farmers & biofuels producers after Trump’s latest insult to ethanol industry,\" Sept. 15, 2020'},\n",
       "  {'link': 'https://www.reuters.com/article/us-usa-biofuels/u-s-epa-grants-three-biofuel-waivers-to-refiners-before-trump-leaves-office-idUSKBN29O2SE',\n",
       "   'description': 'Reuters, \"U.S. EPA grants three biofuel waivers to refiners before Trump leaves office,\" Jan. 19, 2021'},\n",
       "  {'link': 'https://millermeeks.house.gov/media/press-releases/miller-meeks-urges-president-biden-keep-promises-biofuels-producers',\n",
       "   'description': 'Rep. Mariannette Miller-Meeks news release, \"Miller-Meeks Urges President Biden to Keep Promises to Biofuels Producers, Sept. 22, 2021'},\n",
       "  {'link': 'http://biomassmagazine.com/articles/18025/iowa-lawmakers-urge-biden-to-prioritize-biofuels',\n",
       "   'description': 'Biomass Magazine, \"Iowa lawmakers urge Biden to prioritize biofuels,\" by Erin Voegele, May 25, 2021'},\n",
       "  {'link': 'https://www.iowaagribusinessradionetwork.com/iowa-lawmakers-holding-president-biden-to-his-biofuels-promise/',\n",
       "   'description': 'Iowa Agribusiness Radio Network, \"Iowa lawmakers holding President Biden to his biofuels promise,\" by Riley Smith, May 31, 2021'},\n",
       "  {'link': 'https://www.desmoinesregister.com/story/news/politics/2021/04/21/iowa-rep-cindy-axne-pushes-add-ethanol-biodiesel-measures-to-president-joe-biden-infrastructure-deal/7284346002/',\n",
       "   'description': 'Des Moines Register, \"U.S. Rep. Cindy Axne pushes to include ethanol measures in $2 trillion infrastructure deal,\" by Brianne Pfannenstiel, April 21, 2021'},\n",
       "  {'link': 'https://biofuels-news.com/news/ad-campaign-urges-biden-not-to-make-rfs-u-turn/',\n",
       "   'description': 'Biofuels International, \"Ad campaign urges Biden not to make RFS U-turn,\" Aug. 13, 2021'},\n",
       "  {'link': 'https://www.hydrocarbonprocessing.com/news/2021/09/eia-releases-plant-level-us-biofuels-production-capacity-data',\n",
       "   'description': 'HydrocarbonProcessing.com, \"EIA releases plant-level U.S. biofuels production capacity data,\" Sept. 13, 2021'},\n",
       "  {'link': 'https://www.eia.gov/petroleum/ethanolcapacity/',\n",
       "   'description': 'U.S. Energy Information Administration, \"U.S. Fuel Ethanol Plant Production Capacity,\" Sept. 3, 2021'}],\n",
       " 'country_of_origin': 'usa',\n",
       " 'label': 'True',\n",
       " 'published': '2021-10-28',\n",
       " 'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n",
       " 'crawled_date': '2022-10-06T21:00:06',\n",
       " 'speaker': 'Randy Feenstra',\n",
       " 'factchecker': 'Sabine Martin',\n",
       " 'topic': ['Climate Change',\n",
       "  'Energy',\n",
       "  'Transportation',\n",
       "  'Iowa',\n",
       "  'Randy Feenstra'],\n",
       " 'lang': 'en',\n",
       " 'fact_source': 'politifact',\n",
       " 'supported': True,\n",
       " 'gpt3_claim': 'The Biden administration published a study concluding 4 of 5 new cars on the road by 2050 will still require liquid fuels.',\n",
       " 'gpt3_label': 'Partially True',\n",
       " 'gpt3_queries': ['Biden administration study on liquid fuels in new cars by 2050',\n",
       "  \"U.S. Energy Information Administration's 2021 Annual Energy Outlook report\",\n",
       "  'Future of automobile industry and liquid fuels'],\n",
       " 'gpt3_summary': \"The U.S. Energy Information Administration's 2021 Annual Energy Outlook report projects that 79% of new vehicles sold will still use liquid fuels in 2050, accounting for 95% of sales in 2020. A report from the Biden administration was cited to support the claim that 4 out of 5 new cars will require liquid fuels by 2050. Biden did not provide a timeline for when electric power would come close to the share of the automobile market so it is not clear whether the claim is true, false or partially true/false.\",\n",
       " 'label_original': 'true',\n",
       " 'summary': 'U.S. Rep. Randy Feenstra, R-Iowa, tweeted that four out of five new cars in 2050 will still require liquid fuels, and that the projection contradicts an Oct. 5 statement from President Joe Biden touting electric as an energy source for automobiles.  The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report states that a majority, 79%, of vehicles will have liquid fuel by 2050. Ethanol consumption is important in Iowa because the state leads the nation in ethanol production.',\n",
       " 'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n",
       " 'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n",
       " 'complexity_class': 'complex',\n",
       " 'numerical': True,\n",
       " 'taxonomy_label': 'statistical'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cYv6F4Nk4SJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eKFjiC3i8Yx"
   },
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "  features = []\n",
    "  evidences = []\n",
    "\n",
    "  for index, fact in enumerate(data):\n",
    "    claim = fact[\"claim\"]\n",
    "\n",
    "\n",
    "    feature = \"[Claim]:\"+claim+\"[Evidences]:\"+fact[\"doc\"]\n",
    "    features.append(feature)\n",
    "  return features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY7Gux6Yn5rI"
   },
   "outputs": [],
   "source": [
    "train_features = get_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ5AoF54oV2t",
    "outputId": "77cf02d1-d04d-4200-b269-a1d6769c2038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9935"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "FHml9DUHn_bh",
    "outputId": "d9aa891b-3b44-40b4-d955-b36237584a76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister\\'s statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel\\'s screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeHEwL3doMD6"
   },
   "outputs": [],
   "source": [
    "val_features = get_features(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQ-ipHaHoiY-",
    "outputId": "b8326a13-bc9f-4d94-b0d4-6c572b6551d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "Iyh75sm2ok_6",
    "outputId": "ebdcbc0d-2a1d-415d-8eaf-219feadadc1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[Claim]:Video of show Pakistani players celebrating the rain-hit match against India in Asia Cup 2023[Evidences]:The first Asia Cup encounter between India and Pakistan was washed away by rain on September 2, 2023. Against this backdrop, a viral video showing Pakistani players celebrating in the dressing room is doing the rounds on social media. Many users claim that players were celebrating cancellation of the match due to rain.A Facebook user posted the viral post with a caption:मैं तो कुछ और ही सोचा था कल एक पोस्ट भी की लेकिन ये तो सुपर फट्टू निकलेबारिश के कारण मैच रद्द होने के बाद पाकिस्तान के ड्रेसिंग रूम से लीक हुआ फुटेज:#INDvPAK | #AsiaCup2023(English translation: I was thinking something else yesterday I posted a clip but it turned out to be super stupid Footage leaked from Pakistan's dressing room after match was cancelled due to rain: #INDvPAK | #AsiaCup2023)You can check the post here.FACT CHECKNewsMobile fact-checked the viral post, and found it to be misleading.Running a Reverse Image Search of the video keyframes, the NM team identified a tweet on the official handle of Pakistan Cricket, dated September 2022. The video looks like a longer version of the viral post. As per its caption, the clip is from Pakistan's win over India in the 2022 Asia Cup.The same video was also published on the official YouTube channel of Pakistan Cricket on September 4, 2022. This clarifies that it has nothing to do with the ongoing Asia Cup 2023.Firstpost, too, reported on the raw celebrations by Pakistani players after beating India in the last-over thriller in Asia Cup 2022.Therefore, we can conclusively say that the viral video, claiming to show Pakistani players celebrating the rain-hit match against India in Asia Cup 2023, is misleading.If you want to fact-check any story, WhatsApp it now on +91 11 7127 9799FAKE NEWS BUSTER Name Email Phone Picture/video Picture/video url Description ΔClick here for Latest News updates and viral videos on our AI-powered smart news\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98BSyfIZontr"
   },
   "outputs": [],
   "source": [
    "train_labels = [fact[\"label\"] for fact in train_data]\n",
    "val_labels = [fact[\"label\"] for fact in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJYf-sJ4pHCd",
    "outputId": "77bb17b3-284b-4854-ce83-b2f344f11c0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_final = LE.fit_transform(train_labels)\n",
    "train_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cjPop6cpdiA",
    "outputId": "58e7b625-980f-424c-91f0-a7d1436f2566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_final[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dO586pApTPH",
    "outputId": "c5837a6e-bc2b-43aa-a6f2-a9ee8cf9ddd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels_final = LE.transform(val_labels)\n",
    "val_labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDzDVC1lnmwD",
    "outputId": "317fbe01-8dea-4900-e684-f6d3265c170e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stated_in': 'stated on October 5, 2021 in a tweet',\n",
       " 'date_stated': '2021-10-05',\n",
       " 'sources': [{'link': 'https://twitter.com/RepFeenstra/status/1445480085243367427?s=20',\n",
       "   'description': 'Twitter, Rep. Randy Feenstra, Oct. 5, 2021'},\n",
       "  {'link': 'https://twitter.com/JenniferJJacobs/status/1445477075947646978?s=20',\n",
       "   'description': 'Twitter, Jennifer Jacobs, Oct. 5, 2021'},\n",
       "  {'link': 'https://t.co/Qk1Chwurc3?amp=1',\n",
       "   'description': 'U.S. Energy Information Administration, Annual Energy Outlook 2021, 2021'},\n",
       "  {'link': 'https://www.desmoinesregister.com/story/money/agriculture/2020/09/14/epa-says-reject-requests-cut-ethanol-use/5790852002/',\n",
       "   'description': 'The Des Moines Register, \"With rural voters’ support at risk, Trump administration backs most ethanol mandates,\" by Donnelle Eller, Sept. 14, 2020'},\n",
       "  {'link': 'https://joebiden.com/2020/09/15/statement-by-vice-president-joe-biden-on-need-to-stand-with-farmers-and-biofuel-producers-after-donald-trumps-latest-insult-to-ethanol-industry/',\n",
       "   'description': 'Biden-Harris campaign, \"Statement by VP Biden on standing with farmers & biofuels producers after Trump’s latest insult to ethanol industry,\" Sept. 15, 2020'},\n",
       "  {'link': 'https://www.reuters.com/article/us-usa-biofuels/u-s-epa-grants-three-biofuel-waivers-to-refiners-before-trump-leaves-office-idUSKBN29O2SE',\n",
       "   'description': 'Reuters, \"U.S. EPA grants three biofuel waivers to refiners before Trump leaves office,\" Jan. 19, 2021'},\n",
       "  {'link': 'https://millermeeks.house.gov/media/press-releases/miller-meeks-urges-president-biden-keep-promises-biofuels-producers',\n",
       "   'description': 'Rep. Mariannette Miller-Meeks news release, \"Miller-Meeks Urges President Biden to Keep Promises to Biofuels Producers, Sept. 22, 2021'},\n",
       "  {'link': 'http://biomassmagazine.com/articles/18025/iowa-lawmakers-urge-biden-to-prioritize-biofuels',\n",
       "   'description': 'Biomass Magazine, \"Iowa lawmakers urge Biden to prioritize biofuels,\" by Erin Voegele, May 25, 2021'},\n",
       "  {'link': 'https://www.iowaagribusinessradionetwork.com/iowa-lawmakers-holding-president-biden-to-his-biofuels-promise/',\n",
       "   'description': 'Iowa Agribusiness Radio Network, \"Iowa lawmakers holding President Biden to his biofuels promise,\" by Riley Smith, May 31, 2021'},\n",
       "  {'link': 'https://www.desmoinesregister.com/story/news/politics/2021/04/21/iowa-rep-cindy-axne-pushes-add-ethanol-biodiesel-measures-to-president-joe-biden-infrastructure-deal/7284346002/',\n",
       "   'description': 'Des Moines Register, \"U.S. Rep. Cindy Axne pushes to include ethanol measures in $2 trillion infrastructure deal,\" by Brianne Pfannenstiel, April 21, 2021'},\n",
       "  {'link': 'https://biofuels-news.com/news/ad-campaign-urges-biden-not-to-make-rfs-u-turn/',\n",
       "   'description': 'Biofuels International, \"Ad campaign urges Biden not to make RFS U-turn,\" Aug. 13, 2021'},\n",
       "  {'link': 'https://www.hydrocarbonprocessing.com/news/2021/09/eia-releases-plant-level-us-biofuels-production-capacity-data',\n",
       "   'description': 'HydrocarbonProcessing.com, \"EIA releases plant-level U.S. biofuels production capacity data,\" Sept. 13, 2021'},\n",
       "  {'link': 'https://www.eia.gov/petroleum/ethanolcapacity/',\n",
       "   'description': 'U.S. Energy Information Administration, \"U.S. Fuel Ethanol Plant Production Capacity,\" Sept. 3, 2021'}],\n",
       " 'country_of_origin': 'usa',\n",
       " 'label': 'True',\n",
       " 'published': '2021-10-28',\n",
       " 'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n",
       " 'crawled_date': '2022-10-06T21:00:06',\n",
       " 'speaker': 'Randy Feenstra',\n",
       " 'factchecker': 'Sabine Martin',\n",
       " 'topic': ['Climate Change',\n",
       "  'Energy',\n",
       "  'Transportation',\n",
       "  'Iowa',\n",
       "  'Randy Feenstra'],\n",
       " 'lang': 'en',\n",
       " 'fact_source': 'politifact',\n",
       " 'supported': True,\n",
       " 'gpt3_claim': 'The Biden administration published a study concluding 4 of 5 new cars on the road by 2050 will still require liquid fuels.',\n",
       " 'gpt3_label': 'Partially True',\n",
       " 'gpt3_queries': ['Biden administration study on liquid fuels in new cars by 2050',\n",
       "  \"U.S. Energy Information Administration's 2021 Annual Energy Outlook report\",\n",
       "  'Future of automobile industry and liquid fuels'],\n",
       " 'gpt3_summary': \"The U.S. Energy Information Administration's 2021 Annual Energy Outlook report projects that 79% of new vehicles sold will still use liquid fuels in 2050, accounting for 95% of sales in 2020. A report from the Biden administration was cited to support the claim that 4 out of 5 new cars will require liquid fuels by 2050. Biden did not provide a timeline for when electric power would come close to the share of the automobile market so it is not clear whether the claim is true, false or partially true/false.\",\n",
       " 'label_original': 'true',\n",
       " 'summary': 'U.S. Rep. Randy Feenstra, R-Iowa, tweeted that four out of five new cars in 2050 will still require liquid fuels, and that the projection contradicts an Oct. 5 statement from President Joe Biden touting electric as an energy source for automobiles.  The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report states that a majority, 79%, of vehicles will have liquid fuel by 2050. Ethanol consumption is important in Iowa because the state leads the nation in ethanol production.',\n",
       " 'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n",
       " 'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n",
       " 'complexity_class': 'complex',\n",
       " 'numerical': True,\n",
       " 'taxonomy_label': 'statistical'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FardkyUtyT2B",
    "outputId": "15fb3904-50db-40d7-a0cc-3a940f9bae27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9xEVEmzpcSS",
    "outputId": "0886d2f7-d1c3-4c7c-9211-afeb6c7fa24c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister's statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel's screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n",
      "Token IDs: tensor([    0, 10975, 45699, 42645,  1121,    69,  1229,  1901,     6,   234,\n",
      "         9856,  2331, 33922,   271,  7243,  1695,    14,     5,  1621,  7664,\n",
      "         1718,     6,   151,  4963, 10918, 27353,    11,     5,   247, 31274,\n",
      "        25377, 31688, 42645, 20328,  4090,   692,   234,  9856,  2331, 33922,\n",
      "          271,  7243,  2026,     5,   168,  7664,  1718,     6,   151,  4963,\n",
      "        10918, 27353,   223,     5,   121,   267,  2331,  3552,   116,   152,\n",
      "           74, 25696,     5,  4698,   213, 26390,   851,    59,  2993, 27353,\n",
      "            7,   358,   621,    11,   666,     4,   497,   513,    42,    16,\n",
      "           99,    16,   145,  1695,    30,   103,   592,   433,  1434,    54,\n",
      "           32,  3565,    10, 27314,    31,    10,   340,  2835,    15,   265,\n",
      "         4238, 17826, 11614,   102,  1222,     4,    20,  1345,   924, 33922,\n",
      "          271,  7243,  5830,    69,  1229,  1901,   150,    10,  3747,    23,\n",
      "            5,  2576,  7005,   111,    22,  2022,     6,   151,  4963, 10918,\n",
      "        32384, 17279,  5285,   179,  5100,   242,   113,    36,  2022,     6,\n",
      "          151,  4963, 10918, 27353,    58,  7664,   322,    20, 24512,  2029,\n",
      "            5,  8450,    14, 33922,   271,  7243,    26,    42,  3645,    11,\n",
      "           69,  1901,     4,  5008, 38839,    32, 18534,  4817,    23,    42,\n",
      "        15846,   346, 13294,    14,     5,  2879,  1269,    18,   445,    16,\n",
      "         1528,     4,   993,  1148,   917,    32,    67, 33220,    69,    30,\n",
      "         3565,     5, 27314,     9,     5,   340,  4238,     4,   125,     6,\n",
      "          666,  2477,  9511, 24530,   491,  1771,  8499,    36,  8573,  8460,\n",
      "           43,   303,    14, 33922,   271,  7243,   393,    26,    14,  1718,\n",
      "            6,   151,  4963, 10918, 27353,    58,  7664,    11,     5,   247,\n",
      "           11,    69,  1901,     4,    96,   754,     6,    79,    26,    14,\n",
      "         2219,  1718,  4963, 10918, 27353,    58,  7664,   223,     5,   121,\n",
      "          267,  2331,  3552,     4,  3687,     2])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in train_features:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train_features[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evSioYADp_y4"
   },
   "outputs": [],
   "source": [
    "train_labels_final = torch.tensor(train_labels_final)\n",
    "val_labels_final = torch.tensor(val_labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Nk-HGh0yWwf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GC7Tt92syW5Z",
    "outputId": "85518608-eb48-435b-a6ec-e4c7944ad10e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3084])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLCxP_iqzspd",
    "outputId": "1cfc2c8b-ead4-4cee-e1b0-3d9583768f2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIo3GxyUrMml",
    "outputId": "ab0e347b-d8df-4202-b2d7-7bb99f399834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['True', 'Half True/False', 'False']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(list(set(train_labels)))\n",
    "list(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wg90AQJcrZFD",
    "outputId": "f90831ed-3370-4693-f87f-405f6fdb628d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOK3P-9Gra78"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
    "# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, train_labels_final)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks,val_labels_final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6ALqnRkrjBN"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "            dataset,  # The training samples.\n",
    "            sampler = RandomSampler(dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKSZPwobrl8y"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.roberta = AutoModel.from_pretrained(bert_model_path,output_hidden_states=True,output_attentions=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(mlp_dim, mlp_dim),\n",
    "            # # nn.ReLU(),\n",
    "            # # nn.Linear(mlp_dim, mlp_dim),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, labels_count)\n",
    "        )\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        if freeze_bert:\n",
    "            print(\"Freezing layers\")\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, tokens, masks):\n",
    "        output = self.roberta(tokens, attention_mask=masks)\n",
    "        dropout_output = self.dropout(output[\"pooler_output\"])\n",
    "        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n",
    "        # concat_output = self.dropout(concat_output)\n",
    "        mlp_output = self.mlp(dropout_output)\n",
    "        # proba = self.sigmoid(mlp_output)\n",
    "        # proba = self.softmax(mlp_output)\n",
    "\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAxysrcNsFl8",
    "outputId": "263ae100-1064-40f2-dcbd-fb702692d9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiClassClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Loads BertForSequenceClassification, the pretrained BERT model with a single\n",
    "model = MultiClassClassifier('roberta-large-mnli',num_classes, 1024,768,140,dropout=0.1,freeze_bert=False)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"model_bert_difficulty_prediction/model_weights\"))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iilNEZRCsJjH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awQ2Y9Jb3kht",
    "outputId": "c23f25c7-230b-42d8-9c80-0f7432e9fec9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ys-M4-e3khv"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrYqErOD3khx",
    "outputId": "22b99e9d-ca60-48d5-ddc6-483124ba098e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWVSE9LM3kh0",
    "outputId": "b2281f5f-716a-4fd8-c03d-29db9c46c368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61920"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1935 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcvxVVi63kh3"
   },
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUw3zm6g3kh5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta6zfUTa3kh7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFq9gd5kQSHb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_nmuoSgQ5t3"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugVDrHu20c8G"
   },
   "outputs": [],
   "source": [
    "for param in model.roberta.encoder.layer[0:5].parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1rDO58zMfc8"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LhAy2hZ3kh9",
    "outputId": "d670686d-962f-4a24-efdb-03fb7cc21c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    621.    Elapsed: 0:02:51.\n",
      "  Batch   120  of    621.    Elapsed: 0:04:23.\n",
      "  Batch   160  of    621.    Elapsed: 0:05:54.\n",
      "  Batch   200  of    621.    Elapsed: 0:07:25.\n",
      "  Batch   240  of    621.    Elapsed: 0:08:56.\n",
      "  Batch   280  of    621.    Elapsed: 0:10:28.\n",
      "  Batch   320  of    621.    Elapsed: 0:11:59.\n",
      "  Batch   360  of    621.    Elapsed: 0:13:31.\n",
      "  Batch   400  of    621.    Elapsed: 0:15:02.\n",
      "  Batch   440  of    621.    Elapsed: 0:16:33.\n",
      "  Batch   480  of    621.    Elapsed: 0:18:05.\n",
      "  Batch   520  of    621.    Elapsed: 0:19:36.\n",
      "  Batch   560  of    621.    Elapsed: 0:21:07.\n",
      "  Batch   600  of    621.    Elapsed: 0:22:39.\n",
      " Train Accuracy: 0.66\n",
      "\n",
      "  Average training loss: 0.73\n",
      "  Training epcoh took: 0:23:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.70\n",
      "Validation loss decreased (inf --> 0.643350).  Saving model ...\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:02:35\n",
      "Saving model to model_roberta_large_oracle/\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:01:32.\n",
      "  Batch    80  of    621.    Elapsed: 0:03:04.\n",
      "  Batch   120  of    621.    Elapsed: 0:04:35.\n",
      "  Batch   160  of    621.    Elapsed: 0:06:07.\n",
      "  Batch   200  of    621.    Elapsed: 0:07:38.\n",
      "  Batch   240  of    621.    Elapsed: 0:09:09.\n",
      "  Batch   280  of    621.    Elapsed: 0:10:41.\n",
      "  Batch   320  of    621.    Elapsed: 0:12:12.\n",
      "  Batch   360  of    621.    Elapsed: 0:13:43.\n",
      "  Batch   400  of    621.    Elapsed: 0:15:15.\n",
      "  Batch   440  of    621.    Elapsed: 0:16:46.\n",
      "  Batch   480  of    621.    Elapsed: 0:18:17.\n",
      "  Batch   520  of    621.    Elapsed: 0:19:49.\n",
      "  Batch   560  of    621.    Elapsed: 0:21:20.\n",
      "  Batch   600  of    621.    Elapsed: 0:22:51.\n",
      " Train Accuracy: 0.73\n",
      "\n",
      "  Average training loss: 0.60\n",
      "  Training epcoh took: 0:23:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.72\n",
      "EarlyStopping counter: 1 out of 2\n",
      "  Validation Loss: 0.64\n",
      "  Validation took: 0:02:32\n",
      "Saving model to model_roberta_large_oracle/\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "  Batch    40  of    621.    Elapsed: 0:01:32.\n",
      "  Batch    80  of    621.    Elapsed: 0:03:03.\n",
      "  Batch   120  of    621.    Elapsed: 0:04:35.\n",
      "  Batch   160  of    621.    Elapsed: 0:06:06.\n",
      "  Batch   200  of    621.    Elapsed: 0:07:37.\n",
      "  Batch   240  of    621.    Elapsed: 0:09:08.\n",
      "  Batch   280  of    621.    Elapsed: 0:10:39.\n",
      "  Batch   320  of    621.    Elapsed: 0:12:11.\n",
      "  Batch   360  of    621.    Elapsed: 0:13:42.\n",
      "  Batch   400  of    621.    Elapsed: 0:15:13.\n",
      "  Batch   440  of    621.    Elapsed: 0:16:45.\n",
      "  Batch   480  of    621.    Elapsed: 0:18:16.\n",
      "  Batch   520  of    621.    Elapsed: 0:19:47.\n",
      "  Batch   560  of    621.    Elapsed: 0:21:18.\n",
      "  Batch   600  of    621.    Elapsed: 0:22:50.\n",
      " Train Accuracy: 0.77\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epcoh took: 0:23:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:18:41 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to\n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_poincare = batch[2].to(device)\n",
    "        # b_difficulty = batch[3].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # skill_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because\n",
    "        # accumulating the gradients is \"convenient while training RNNs\".\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        probas = model(b_input_ids,b_input_mask)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value\n",
    "        # from the tensor.\n",
    "        loss = loss_func(probas, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        # scheduler.step()\n",
    "        logits = probas.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        # b_poincare = batch[2].to(device)\n",
    "        # b_difficulty = batch[3].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # skill_labels = batch[3].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "\n",
    "          logits = model(b_input_ids,b_input_mask)\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        loss = loss_func(logits, b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "      print(\"Early stopping\")\n",
    "      break\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    output_dir = 'model_roberta_large_oracle/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
    "\n",
    "    !rm -rf \"/content/drive/My Drive/ecir_compnumfacts/model_roberta_large_oracle\"\n",
    "    !mv model_roberta_large_oracle \"/content/drive/My Drive/ecir_compnumfacts/\"\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y85_jMDiw9uP",
    "outputId": "3d185869-d3c4-4d52-9007-d894d3dc6427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEEKmPey0a8-"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "33cc7df771c845bd869d6b5124f2604d",
      "794716544ed3491aba47432a4f3afca4",
      "f64930120a7748eaa8a743c77e52cc4d",
      "931cbe93dc6546ba970a79e0cbf678bd",
      "18b0770757c241de8253b1d94ee568fe",
      "0d7e62772c6f451d9575dd5df95d879c",
      "89226fc74b2c44a2978c4d0b43b60739",
      "cf150bd16c424916a8ba79e763d07f1d",
      "469f372a38644513abf1374edc48ee3d",
      "ad973a985d3a421eaa5c1a7ca55420ed",
      "71e781b197b5496e8809b7fff046a20e",
      "8bdd37fccf354d9793047799a966f2a0",
      "39317c1aa3694777947eab03b2dd3ff5",
      "c71f8c507eab42e3baa08939664d3029",
      "804dbceb1d564e0e8a0fdc5e57f2c489",
      "cbb79f0a4ca246a18381b836665264e4",
      "089e03c3f97749b99e6493cd58176603",
      "f6339b7381c54dda8b7876bca36c3054",
      "31e6b85700304187bf5197da351a125e",
      "248d065f02eb4f57a33b60df8dd9fb24",
      "f07654bb49c64b1aa30940181d5e4574",
      "34f0c3999b5c4bf295102050cb6052d4",
      "a76ca4bdf1784163878dae276beca5d5",
      "cef13963e753410c8812b92daebdd48f",
      "169e51f8ece34774baceefbbdff5276d",
      "fb7ce11bf3a14bc3bbca96cbbb4f45ef",
      "521fe75d24e2402f9a4566d82b4602fd",
      "6991b5be37164ec09eb84d9935c4a1f5",
      "f0610958751c4fcba8cd0a042c79dcd0",
      "261eecd1fd6e4b17a9e9bcb574bc9f5b",
      "0de8bc5290ba41a78ab6cac1a9e99c13",
      "0c92603a817143ab8b4bf460a09f3245"
     ]
    },
    "id": "MFR-s_8ZtATC",
    "outputId": "ba9e4dfc-d784-4191-fe58-36b5b97a648d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cc7df771c845bd869d6b5124f2604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWkkt0F4Ifwc"
   },
   "outputs": [],
   "source": [
    "LE.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy__BpY8VOQp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "089e03c3f97749b99e6493cd58176603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c92603a817143ab8b4bf460a09f3245": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d7e62772c6f451d9575dd5df95d879c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbb79f0a4ca246a18381b836665264e4",
      "placeholder": "​",
      "style": "IPY_MODEL_089e03c3f97749b99e6493cd58176603",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "0de8bc5290ba41a78ab6cac1a9e99c13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "169e51f8ece34774baceefbbdff5276d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18b0770757c241de8253b1d94ee568fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_c71f8c507eab42e3baa08939664d3029",
      "style": "IPY_MODEL_804dbceb1d564e0e8a0fdc5e57f2c489",
      "tooltip": ""
     }
    },
    "248d065f02eb4f57a33b60df8dd9fb24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "261eecd1fd6e4b17a9e9bcb574bc9f5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31e6b85700304187bf5197da351a125e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33cc7df771c845bd869d6b5124f2604d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f07654bb49c64b1aa30940181d5e4574",
       "IPY_MODEL_34f0c3999b5c4bf295102050cb6052d4",
       "IPY_MODEL_a76ca4bdf1784163878dae276beca5d5",
       "IPY_MODEL_cef13963e753410c8812b92daebdd48f"
      ],
      "layout": "IPY_MODEL_89226fc74b2c44a2978c4d0b43b60739"
     }
    },
    "34f0c3999b5c4bf295102050cb6052d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_521fe75d24e2402f9a4566d82b4602fd",
      "placeholder": "​",
      "style": "IPY_MODEL_6991b5be37164ec09eb84d9935c4a1f5",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "39317c1aa3694777947eab03b2dd3ff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "469f372a38644513abf1374edc48ee3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "521fe75d24e2402f9a4566d82b4602fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6991b5be37164ec09eb84d9935c4a1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71e781b197b5496e8809b7fff046a20e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "794716544ed3491aba47432a4f3afca4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf150bd16c424916a8ba79e763d07f1d",
      "placeholder": "​",
      "style": "IPY_MODEL_469f372a38644513abf1374edc48ee3d",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "804dbceb1d564e0e8a0fdc5e57f2c489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "89226fc74b2c44a2978c4d0b43b60739": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "8bdd37fccf354d9793047799a966f2a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "931cbe93dc6546ba970a79e0cbf678bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_8bdd37fccf354d9793047799a966f2a0",
      "style": "IPY_MODEL_39317c1aa3694777947eab03b2dd3ff5",
      "value": true
     }
    },
    "a76ca4bdf1784163878dae276beca5d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0610958751c4fcba8cd0a042c79dcd0",
      "placeholder": "​",
      "style": "IPY_MODEL_261eecd1fd6e4b17a9e9bcb574bc9f5b",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "ad973a985d3a421eaa5c1a7ca55420ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c71f8c507eab42e3baa08939664d3029": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbb79f0a4ca246a18381b836665264e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cef13963e753410c8812b92daebdd48f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0de8bc5290ba41a78ab6cac1a9e99c13",
      "placeholder": "​",
      "style": "IPY_MODEL_0c92603a817143ab8b4bf460a09f3245",
      "value": "Login successful"
     }
    },
    "cf150bd16c424916a8ba79e763d07f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0610958751c4fcba8cd0a042c79dcd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f07654bb49c64b1aa30940181d5e4574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_169e51f8ece34774baceefbbdff5276d",
      "placeholder": "​",
      "style": "IPY_MODEL_fb7ce11bf3a14bc3bbca96cbbb4f45ef",
      "value": "Token is valid (permission: write)."
     }
    },
    "f6339b7381c54dda8b7876bca36c3054": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31e6b85700304187bf5197da351a125e",
      "placeholder": "​",
      "style": "IPY_MODEL_248d065f02eb4f57a33b60df8dd9fb24",
      "value": "Connecting..."
     }
    },
    "f64930120a7748eaa8a743c77e52cc4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_ad973a985d3a421eaa5c1a7ca55420ed",
      "placeholder": "​",
      "style": "IPY_MODEL_71e781b197b5496e8809b7fff046a20e",
      "value": ""
     }
    },
    "fb7ce11bf3a14bc3bbca96cbbb4f45ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
