{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T15:59:10.521988Z",
     "start_time": "2025-01-13T15:59:10.515303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.common import read_data, save_data, DATA_PATH, QT_VERACITY_LABELS\n",
    "from src.evidence_processor import EvidenceProcessor"
   ],
   "id": "e304fb724a90d720",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Finding top evidences for decomposed questions / claim",
   "id": "c31464035131191d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-13T17:07:24.640658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "processor = EvidenceProcessor(decomposed=True, top_k=1)\n",
    "\n",
    "DECOMPOSITION = 'gpt3.5-turbo'\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    claims = read_data(f'{DECOMPOSITION}/{split}_decomposed_{DECOMPOSITION}.json')\n",
    "    claims = processor.transform(claims)\n",
    "    save_data(f'{DECOMPOSITION}/{split}_evidences_decomposed_{DECOMPOSITION}.json', claims)"
   ],
   "id": "262ec9556a5d35cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3084 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cecd62c3431c47548baff11fe3f17b37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assinging top100 evidences to claims",
   "id": "f077fddf261859d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_file, evidences_file):\n",
    "    claims = read_data(claims_file)\n",
    "    evidences = read_data(evidences_file)\n",
    "\n",
    "    evidences = {e['claim']: e for e in evidences}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['top100evidences'] = evidences[claim['claim']]['top100evidences']\n",
    "\n",
    "    print(len(claims), len(evidences))\n",
    "    save_data(f'processed_{claims_file}', claims)\n",
    "\n",
    "\n",
    "process('train_claims_quantemp.json', 'train_evidences.json')\n",
    "process('val_claims_quantemp.json', 'val_evidences.json')\n",
    "process('test_claims_quantemp.json', 'test_evidences.json')"
   ],
   "id": "3608ee1567d5c543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing format for gpt3.5-turbo decomposition",
   "id": "7c81c446c8030f98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:10:54.820223Z",
     "start_time": "2025-01-13T11:10:53.117738Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "%autoreload\n",
    "\n",
    "claims = read_data(f'raw_data/test_claims_evidences.json')\n",
    "decomposed_questions = pd.read_csv(f'{DATA_PATH}/test_claimdecomp.csv', sep=\"@\")\n",
    "\n",
    "for claim in claims:#\n",
    "    questions = decomposed_questions[decomposed_questions['claims'] == claim['claim']]['questions']\n",
    "\n",
    "    if len(questions) == 0:\n",
    "        questions = []\n",
    "    elif len(questions) == 1:\n",
    "        questions = questions.iloc[0].split(\"Next Question: \")\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "    questions = [q.strip() for q in questions]\n",
    "    claim['questions'] = questions\n",
    "\n",
    "\n",
    "with open(f'{DATA_PATH}/test_decomposed_gpt3.5-turbo.json', \"w\") as file:\n",
    "    json.dump(claims, file, indent=4)"
   ],
   "id": "f9e723661c6f5f0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing label names in gpt3.5-turbo decomposition",
   "id": "75571c2fc8ee2cba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:28:47.131079Z",
     "start_time": "2025-01-13T11:28:46.068541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "%autoreload\n",
    "\n",
    "def fix_labels(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        if claim['label'] == \"Half True/False\":\n",
    "            claim['label'] = \"Conflicting\"\n",
    "\n",
    "        assert claim['label'] in QT_VERACITY_LABELS\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'fixed_{filename}', claims)\n",
    "\n",
    "\n",
    "fix_labels('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('test_evidences_decomposed_gpt3.5-turbo.json')\n"
   ],
   "id": "4fb58b6af2718cd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Extract nested evidences to questions and evidences lists",
   "id": "e9157957e7dc50c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:18:38.804608Z",
     "start_time": "2025-01-13T14:18:36.223231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def extract(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        questions = [e['questions'] for e in claim['evidences']]\n",
    "\n",
    "        del claim['evidences']\n",
    "        claim['questions'] = questions\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'{DATA_PATH}/fixed_{filename}', claims)\n",
    "\n",
    "extract('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('test_evidences_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "10697d7d1c8c2618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assigning gpt3.5-turbo decomposition questions to",
   "id": "a68a4d2ab2401844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:54:41.325882Z",
     "start_time": "2025-01-13T14:54:37.853954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_filename, questions_filename):\n",
    "    claims = read_data(f'raw_data/{claims_filename}')\n",
    "    questions = read_data(f'gpt3.5-turbo/{questions_filename}')\n",
    "    print(len(claims), len(questions))\n",
    "\n",
    "    questions_dict = {q['claim']: q['questions'] for q in questions}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['questions'] = questions_dict[claim['claim']]\n",
    "\n",
    "    save_data(f'gpt3.5-turbo/processed_{claims_filename}', claims)\n",
    "\n",
    "\n",
    "process('train_claims.json', 'train_decomposed_gpt3.5-turbo.json')\n",
    "process('val_claims.json', 'val_decomposed_gpt3.5-turbo.json')\n",
    "process('test_claims.json', 'test_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "496dacefc2573df8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935 9935\n",
      "3084 3084\n",
      "2495 2495\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Rename field key",
   "id": "1c9bc36af7924011"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T15:27:31.305004Z",
     "start_time": "2025-01-13T15:27:26.446225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rename_key(path, filename, old_key, new_key):\n",
    "    claims = read_data(os.path.join(path, filename))\n",
    "\n",
    "    for claim in claims:\n",
    "        claim[new_key] = claim.pop(old_key)\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(os.path.join(path, f'renamed_{filename}'), claims)\n",
    "\n",
    "path = 'gpt3.5-turbo'\n",
    "rename_key(path, 'train_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'val_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'test_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')"
   ],
   "id": "5c41f0c0a1220a5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove field",
   "id": "6512903e4cfefbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_field(path, filename, key):\n",
    "    claims = read_data(os.path.join(path, filename))\n",
    "\n",
    "    for claim in claims:\n",
    "        del claim[key]\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(os.path.join(path, f'processed_{filename}'), claims)\n",
    "\n",
    "path = 'gpt3.5-turbo'\n",
    "remove_field(path, 'train_decomposed_gpt3.5-turbo.json', 'doc')"
   ],
   "id": "ad3c1dce997c36b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
