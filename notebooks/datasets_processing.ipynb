{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.common import read_data, save_data, DATA_PATH, QT_VERACITY_LABELS\n",
    "from src.evidence_processor import EvidenceProcessor\n",
    "from collections import Counter"
   ],
   "id": "e304fb724a90d720",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Finding top evidences for decomposed questions / claim",
   "id": "c31464035131191d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "processor = EvidenceProcessor(decomposed=False)\n",
    "\n",
    "DECOMPOSITION = 'flant5'\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    claims = read_data(f'{DECOMPOSITION}/{split}_decomposed_{DECOMPOSITION}.json')\n",
    "    claims = processor.transform(claims)\n",
    "    save_data(f'{DECOMPOSITION}/{split}_evidences_decomposed_{DECOMPOSITION}.json', claims)\n",
    "    # save_data(f'{DECOMPOSITION}/{split}_evidences_decomposed_{DECOMPOSITION}.json', claims)"
   ],
   "id": "262ec9556a5d35cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assinging top100 evidences to claims",
   "id": "f077fddf261859d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_file, evidences_file):\n",
    "    claims = read_data(claims_file)\n",
    "    evidences = read_data(evidences_file)\n",
    "\n",
    "    evidences = {e['claim']: e for e in evidences}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['top100evidences'] = evidences[claim['claim']]['top100evidences']\n",
    "\n",
    "    print(len(claims), len(evidences))\n",
    "    save_data(claims_file, claims)\n",
    "\n",
    "\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     process(f'flant5/{split}_decomposed_flant5.json', f'raw_data/{split}_claims.json')\n",
    "\n",
    "# process('train_claims_quantemp.json', 'train_evidences.json')\n",
    "# process('val_claims_quantemp.json', 'val_evidences.json')\n",
    "# process('test_claims_quantemp.json', 'test_evidences.json')"
   ],
   "id": "3608ee1567d5c543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(read_data('raw_data/train_claims.json')))\n",
   "id": "98ea51fee5ccf332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing format for gpt3.5-turbo decomposition",
   "id": "7c81c446c8030f98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "claims = read_data(f'raw_data/test_claims_evidences.json')\n",
    "decomposed_questions = pd.read_csv(f'{DATA_PATH}/test_claimdecomp.csv', sep=\"@\")\n",
    "\n",
    "for claim in claims:#\n",
    "    questions = decomposed_questions[decomposed_questions['claims'] == claim['claim']]['questions']\n",
    "\n",
    "    if len(questions) == 0:\n",
    "        questions = []\n",
    "    elif len(questions) == 1:\n",
    "        questions = questions.iloc[0].split(\"Next Question: \")\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "    questions = [q.strip() for q in questions]\n",
    "    claim['questions'] = questions\n",
    "\n",
    "\n",
    "with open(f'{DATA_PATH}/test_decomposed_gpt3.5-turbo.json', \"w\") as file:\n",
    "    json.dump(claims, file, indent=4)"
   ],
   "id": "f9e723661c6f5f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing label names in gpt3.5-turbo decomposition",
   "id": "75571c2fc8ee2cba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def fix_labels(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        if claim['label'] == \"Half True/False\":\n",
    "            claim['label'] = \"Conflicting\"\n",
    "\n",
    "        assert claim['label'] in QT_VERACITY_LABELS\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'fixed_{filename}', claims)\n",
    "\n",
    "\n",
    "fix_labels('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('test_evidences_decomposed_gpt3.5-turbo.json')\n"
   ],
   "id": "4fb58b6af2718cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Extract nested evidences to questions and evidences lists",
   "id": "e9157957e7dc50c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def extract(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        questions = [e['questions'] for e in claim['evidences']]\n",
    "\n",
    "        del claim['evidences']\n",
    "        claim['questions'] = questions\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'{DATA_PATH}/fixed_{filename}', claims)\n",
    "\n",
    "extract('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('test_evidences_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "10697d7d1c8c2618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assigning gpt3.5-turbo decomposition questions to",
   "id": "a68a4d2ab2401844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_filename, questions_filename):\n",
    "    claims = read_data(f'raw_data/{claims_filename}')\n",
    "    questions = read_data(f'gpt3.5-turbo/{questions_filename}')\n",
    "    print(len(claims), len(questions))\n",
    "\n",
    "    questions_dict = {q['claim']: q['questions'] for q in questions}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['questions'] = questions_dict[claim['claim']]\n",
    "\n",
    "    save_data(f'gpt3.5-turbo/processed_{claims_filename}', claims)\n",
    "\n",
    "\n",
    "process('train_claims.json', 'train_decomposed_gpt3.5-turbo.json')\n",
    "process('val_claims.json', 'val_decomposed_gpt3.5-turbo.json')\n",
    "process('test_claims.json', 'test_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "496dacefc2573df8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Rename field key",
   "id": "1c9bc36af7924011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rename_key(path, filename, old_key, new_key):\n",
    "    claims = read_data(os.path.join(path, filename))\n",
    "\n",
    "    for claim in claims:\n",
    "        claim[new_key] = claim.pop(old_key)\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(os.path.join(path, f'renamed_{filename}'), claims)\n",
    "\n",
    "path = 'gpt3.5-turbo'\n",
    "rename_key(path, 'train_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'val_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'test_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')"
   ],
   "id": "5c41f0c0a1220a5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove field",
   "id": "6512903e4cfefbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_field(filename, key):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        del claim[key]\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'{filename}.JSON', claims)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    remove_field(f'custom_decomposition/{split}_decomposed_flant5_predicted_type.json', 'top100evidences')"
   ],
   "id": "ad3c1dce997c36b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fix missing top100evidences and doc fields",
   "id": "6fb79ccf99383430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = read_data('gpt3.5-turbo/train_decomposed_gpt3.5-turbo.json')\n",
    "raw = read_data('raw_data/train_claims.json')\n",
    "\n",
    "for d, r in zip(data, raw):\n",
    "    d['top100evidences'] = r['top100evidences']\n",
    "    d['doc'] = r['doc']\n",
    "\n",
    "save_data('gpt3.5-turbo/fixed_train_decomposed_gpt3.5-turbo.json', data)"
   ],
   "id": "21b5b62ab628b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add indentation to json files",
   "id": "70e8c7b176a91c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "# train_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "# val_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "\n",
    "name = 'flant5/test_flant5_claimdecomp_subquestions_evidences_processed.json'\n",
    "claims = read_data(name)\n",
    "save_data(name, claims)\n"
   ],
   "id": "5634650f0b65f5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add taxonomy labels",
   "id": "85b94d2645159a7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def attach_taxonomy_labels(filename, raw_filename):\n",
    "    claims = read_data(filename)\n",
    "    raw_claims = read_data(raw_filename)\n",
    "\n",
    "    raw_claims_dict = {claim['claim']: claim['taxonomy_label'] for claim in raw_claims}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['taxonomy_label'] = raw_claims_dict[claim['claim']]\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'{filename}.json', claims)\n",
    "\n",
    "attach_taxonomy_labels(f'flant5/train_decomposed_flant5.json', 'raw_data/train_claims.json')\n",
    "attach_taxonomy_labels(f'flant5/val_decomposed_flant5.json', 'raw_data/val_claims.json')\n",
    "attach_taxonomy_labels(f'flant5/test_decomposed_flant5.json', 'raw_data/test_claims.json')"
   ],
   "id": "40f323ea205d09f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get examples for each category",
   "id": "f9bc095ee47f4738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = read_data(\"raw_data/train_claims.json\")\n",
    "\n",
    "types = [p['taxonomy_label'].strip() for p in data]\n",
    "counter = Counter(types)\n",
    "\n",
    "def save_examples(data, category):\n",
    "    examples = [p['claim'] for p in data if p['taxonomy_label'].strip() == category][:20]\n",
    "    with open(os.path.join(DATA_PATH, \"custom_decomposition\", f'{category}.txt'), \"w\") as file:\n",
    "        file.write(\"\\n\".join(examples))\n",
    "\n",
    "for category in counter:\n",
    "    print(f'{category}: {counter[category]}')\n",
    "    save_examples(data, category)"
   ],
   "id": "834ed97adae8746e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Progate taxonomy_label field from decomposed to evidences_decomposed",
   "id": "6e73d198a0a63b51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def propagate_taxonomy_labels(filename, target_filename):\n",
    "    claims = read_data(filename)\n",
    "    target_claims = read_data(target_filename)\n",
    "\n",
    "    for claim, target_claim in zip(claims, target_claims):\n",
    "        target_claim['taxonomy_label'] = claim['taxonomy_label']\n",
    "\n",
    "    print(len(claims), len(target_claims))\n",
    "    save_data(f'{target_filename}.json', target_claims)\n",
    "\n",
    "propagate_taxonomy_labels('flant5/train_decomposed_flant5.json', 'flant5/train_evidences_decomposed_flant5.json')\n",
    "propagate_taxonomy_labels('flant5/val_decomposed_flant5.json', 'flant5/val_evidences_decomposed_flant5.json')\n",
    "propagate_taxonomy_labels('flant5/test_decomposed_flant5.json', 'flant5/test_evidences_decomposed_flant5.json')\n"
   ],
   "id": "7530f1aa0734f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assign evidences field\n",
   "id": "4933a870fdd1b147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def propagate(filename, target_filename):\n",
    "    claims = read_data(filename)\n",
    "    target_claims = read_data(target_filename)\n",
    "\n",
    "    for claim, target_claim in zip(claims, target_claims):\n",
    "        target_claim['evidences'] = claim['evidences']\n",
    "\n",
    "    print(len(claims), len(target_claims))\n",
    "    save_data(f'{target_filename}.json', target_claims)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    propagate(f'flant5/{split}_evidences_decomposed_flant5.json', f'custom_decomposition/{split}_decomposed_flant5_predicted_type.json')\n"
   ],
   "id": "6efb486c36c8cb85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
