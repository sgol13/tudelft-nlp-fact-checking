{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T11:59:38.956576Z",
     "start_time": "2025-01-14T11:59:34.959109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.common import read_data, save_data, DATA_PATH, QT_VERACITY_LABELS\n",
    "from src.evidence_processor import EvidenceProcessor"
   ],
   "id": "e304fb724a90d720",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Finding top evidences for decomposed questions / claim",
   "id": "c31464035131191d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T23:20:34.855075Z",
     "start_time": "2025-01-14T21:47:49.639086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "processor = EvidenceProcessor(decomposed=False)\n",
    "\n",
    "DECOMPOSITION = 'flant5'\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    claims = read_data(f'{DECOMPOSITION}/{split}_decomposed_{DECOMPOSITION}.json')\n",
    "    claims = processor.transform(claims)\n",
    "    save_data(f'{DECOMPOSITION}/{split}_evidences_decomposed_{DECOMPOSITION}.json', claims)\n",
    "    # save_data(f'{DECOMPOSITION}/{split}_evidences_decomposed_{DECOMPOSITION}.json', claims)"
   ],
   "id": "262ec9556a5d35cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/9935 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac4b52f138b24794877a27797d3c4826"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3084 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac80e577f9514c82bd387cbf4304b32e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2495 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e232ea3f5534a32aa35e9b46e983c06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assinging top100 evidences to claims",
   "id": "f077fddf261859d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:39:51.715416Z",
     "start_time": "2025-01-14T21:39:44.043762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_file, evidences_file):\n",
    "    claims = read_data(claims_file)\n",
    "    evidences = read_data(evidences_file)\n",
    "\n",
    "    evidences = {e['claim']: e for e in evidences}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['top100evidences'] = evidences[claim['claim']]['top100evidences']\n",
    "\n",
    "    print(len(claims), len(evidences))\n",
    "    save_data(claims_file, claims)\n",
    "\n",
    "\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     process(f'flant5/{split}_decomposed_flant5.json', f'raw_data/{split}_claims.json')\n",
    "\n",
    "# process('train_claims_quantemp.json', 'train_evidences.json')\n",
    "# process('val_claims_quantemp.json', 'val_evidences.json')\n",
    "# process('test_claims_quantemp.json', 'test_evidences.json')"
   ],
   "id": "3608ee1567d5c543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935 9929\n",
      "3084 3073\n",
      "2495 2495\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:41:30.326570Z",
     "start_time": "2025-01-14T21:41:29.044226Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(read_data('raw_data/train_claims.json')))\n",
   "id": "98ea51fee5ccf332",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing format for gpt3.5-turbo decomposition",
   "id": "7c81c446c8030f98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:10:54.820223Z",
     "start_time": "2025-01-13T11:10:53.117738Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "%autoreload\n",
    "\n",
    "claims = read_data(f'raw_data/test_claims_evidences.json')\n",
    "decomposed_questions = pd.read_csv(f'{DATA_PATH}/test_claimdecomp.csv', sep=\"@\")\n",
    "\n",
    "for claim in claims:#\n",
    "    questions = decomposed_questions[decomposed_questions['claims'] == claim['claim']]['questions']\n",
    "\n",
    "    if len(questions) == 0:\n",
    "        questions = []\n",
    "    elif len(questions) == 1:\n",
    "        questions = questions.iloc[0].split(\"Next Question: \")\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "    questions = [q.strip() for q in questions]\n",
    "    claim['questions'] = questions\n",
    "\n",
    "\n",
    "with open(f'{DATA_PATH}/test_decomposed_gpt3.5-turbo.json', \"w\") as file:\n",
    "    json.dump(claims, file, indent=4)"
   ],
   "id": "f9e723661c6f5f0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fixing label names in gpt3.5-turbo decomposition",
   "id": "75571c2fc8ee2cba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:28:47.131079Z",
     "start_time": "2025-01-13T11:28:46.068541Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "%autoreload\n",
    "\n",
    "def fix_labels(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        if claim['label'] == \"Half True/False\":\n",
    "            claim['label'] = \"Conflicting\"\n",
    "\n",
    "        assert claim['label'] in QT_VERACITY_LABELS\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'fixed_{filename}', claims)\n",
    "\n",
    "\n",
    "fix_labels('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "fix_labels('test_evidences_decomposed_gpt3.5-turbo.json')\n"
   ],
   "id": "4fb58b6af2718cd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Extract nested evidences to questions and evidences lists",
   "id": "e9157957e7dc50c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:18:38.804608Z",
     "start_time": "2025-01-13T14:18:36.223231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def extract(filename):\n",
    "    claims = read_data(filename)\n",
    "\n",
    "    for claim in claims:\n",
    "        questions = [e['questions'] for e in claim['evidences']]\n",
    "\n",
    "        del claim['evidences']\n",
    "        claim['questions'] = questions\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(f'{DATA_PATH}/fixed_{filename}', claims)\n",
    "\n",
    "extract('train_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('val_evidences_decomposed_gpt3.5-turbo.json')\n",
    "extract('test_evidences_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "10697d7d1c8c2618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assigning gpt3.5-turbo decomposition questions to",
   "id": "a68a4d2ab2401844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:54:41.325882Z",
     "start_time": "2025-01-13T14:54:37.853954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload\n",
    "\n",
    "def process(claims_filename, questions_filename):\n",
    "    claims = read_data(f'raw_data/{claims_filename}')\n",
    "    questions = read_data(f'gpt3.5-turbo/{questions_filename}')\n",
    "    print(len(claims), len(questions))\n",
    "\n",
    "    questions_dict = {q['claim']: q['questions'] for q in questions}\n",
    "\n",
    "    for claim in claims:\n",
    "        claim['questions'] = questions_dict[claim['claim']]\n",
    "\n",
    "    save_data(f'gpt3.5-turbo/processed_{claims_filename}', claims)\n",
    "\n",
    "\n",
    "process('train_claims.json', 'train_decomposed_gpt3.5-turbo.json')\n",
    "process('val_claims.json', 'val_decomposed_gpt3.5-turbo.json')\n",
    "process('test_claims.json', 'test_decomposed_gpt3.5-turbo.json')"
   ],
   "id": "496dacefc2573df8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935 9935\n",
      "3084 3084\n",
      "2495 2495\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Rename field key",
   "id": "1c9bc36af7924011"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T15:27:31.305004Z",
     "start_time": "2025-01-13T15:27:26.446225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rename_key(path, filename, old_key, new_key):\n",
    "    claims = read_data(os.path.join(path, filename))\n",
    "\n",
    "    for claim in claims:\n",
    "        claim[new_key] = claim.pop(old_key)\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(os.path.join(path, f'renamed_{filename}'), claims)\n",
    "\n",
    "path = 'gpt3.5-turbo'\n",
    "rename_key(path, 'train_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'val_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')\n",
    "rename_key(path, 'test_decomposed_gpt3.5-turbo.json', 'questions', 'subquestions')"
   ],
   "id": "5c41f0c0a1220a5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9935\n",
      "3084\n",
      "2495\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove field",
   "id": "6512903e4cfefbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_field(path, filename, key):\n",
    "    claims = read_data(os.path.join(path, filename))\n",
    "\n",
    "    for claim in claims:\n",
    "        del claim[key]\n",
    "\n",
    "    print(len(claims))\n",
    "    save_data(os.path.join(path, f'processed_{filename}'), claims)\n",
    "\n",
    "path = 'flant5'\n",
    "remove_field(path, 'train_decomposed_flant5', 'evidences')"
   ],
   "id": "ad3c1dce997c36b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fix missing top100evidences and doc fields",
   "id": "6fb79ccf99383430"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T19:20:47.415405Z",
     "start_time": "2025-01-14T19:20:41.588059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = read_data('gpt3.5-turbo/train_decomposed_gpt3.5-turbo.json')\n",
    "raw = read_data('raw_data/train_claims.json')\n",
    "\n",
    "for d, r in zip(data, raw):\n",
    "    d['top100evidences'] = r['top100evidences']\n",
    "    d['doc'] = r['doc']\n",
    "\n",
    "save_data('gpt3.5-turbo/fixed_train_decomposed_gpt3.5-turbo.json', data)"
   ],
   "id": "21b5b62ab628b5c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add indentation to json files",
   "id": "70e8c7b176a91c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:59:19.873852Z",
     "start_time": "2025-01-14T20:59:19.652269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "# train_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "# val_flant5_claimdecomp_subquestions_evidences_processed.json\n",
    "\n",
    "name = 'flant5/test_flant5_claimdecomp_subquestions_evidences_processed.json'\n",
    "claims = read_data(name)\n",
    "save_data(name, claims)\n"
   ],
   "id": "5634650f0b65f5b",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/szymong/tud/nlp/fact-checking/data/flan-t5/test_flant5_claimdecomp_subquestions_evidences_processed.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# test_flant5_claimdecomp_subquestions_evidences_processed.json\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# train_flant5_claimdecomp_subquestions_evidences_processed.json\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# val_flant5_claimdecomp_subquestions_evidences_processed.json\u001B[39;00m\n\u001B[1;32m      5\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mflan-t5/test_flant5_claimdecomp_subquestions_evidences_processed.json\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 6\u001B[0m claims \u001B[38;5;241m=\u001B[39m \u001B[43mread_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m save_data(name, claims)\n",
      "File \u001B[0;32m~/tud/nlp/fact-checking/src/common.py:50\u001B[0m, in \u001B[0;36mread_data\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mread_data\u001B[39m(path) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m     49\u001B[0m     abs_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, path)\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mabs_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/szymong/tud/nlp/fact-checking/data/flan-t5/test_flant5_claimdecomp_subquestions_evidences_processed.json'"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
