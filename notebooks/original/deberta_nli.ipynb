{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd1fcb3-9321-4ff3-8b9e-19ff3976ac12",
      "metadata": {
        "id": "4fd1fcb3-9321-4ff3-8b9e-19ff3976ac12",
        "outputId": "367346c1-74f4-472e-a8ea-8b4dd06ab070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sowmy\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import nn\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989ad42a-8a49-48dd-8ebb-89ff515e6dc9",
      "metadata": {
        "id": "989ad42a-8a49-48dd-8ebb-89ff515e6dc9"
      },
      "source": [
        "# Set up model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fda40be-eeb1-4383-b1ff-72c53eb9c41a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fda40be-eeb1-4383-b1ff-72c53eb9c41a",
        "outputId": "df35c99e-210f-4339-d2a7-5912d7b9d34e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ac406f-ac72-416b-b47f-b8e316f023fe",
      "metadata": {
        "id": "e2ac406f-ac72-416b-b47f-b8e316f023fe"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sileod/deberta-v3-base-tasksource-nli')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca21ee41-22c5-47c7-8ae6-5beb9a08d4b1",
      "metadata": {
        "id": "ca21ee41-22c5-47c7-8ae6-5beb9a08d4b1"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00e1dfb-e5fc-4a37-89b5-3b1c0154697f",
      "metadata": {
        "id": "c00e1dfb-e5fc-4a37-89b5-3b1c0154697f"
      },
      "outputs": [],
      "source": [
        "# This dataset is for the decomp training later!\n",
        "# with open(\"../../data/raw_data/train_claimdecomp_evidence_question_mapping.json\") as f:\n",
        "#   train_data = json.load(f)\n",
        "\n",
        "# len(train_data), train_data[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4519182-86fd-489f-8b78-111e42a60981",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4519182-86fd-489f-8b78-111e42a60981",
        "outputId": "fb394366-1c16-4b9e-fe13-483baadc8650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9935,\n",
              " {'crawled_date': '2014-03-26T10:38:09',\n",
              "  'country_of_origin': 'ukraine',\n",
              "  'label': 'False',\n",
              "  'url': 'https://www.stopfake.org/en/fake-commandos-from-berkut-who-refused-to-kneel-have-been-burned-alive-in-lviv/',\n",
              "  'lang': 'en',\n",
              "  'claim': 'FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv',\n",
              "  'doc': 'The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.',\n",
              "  'taxonomy_label': 'statistical',\n",
              "  'label_original': 'FAKE'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"raw_data/train_claims_quantemp.json\") as f:\n",
        "  train_data = json.load(f)\n",
        "\n",
        "len(train_data), train_data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4996af48-d1f2-4af0-80ec-0817c3a15b42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4996af48-d1f2-4af0-80ec-0817c3a15b42",
        "outputId": "40eb03d2-ca46-481b-a23d-9091c42e079b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3084,\n",
              " {'crawled_date': '2022-10-06T21:00:06',\n",
              "  'country_of_origin': 'usa',\n",
              "  'label': 'True',\n",
              "  'url': 'https://www.politifact.com/factchecks/2021/oct/28/randy-feenstra/biden-administration-predicted-liquid-fuel-cars-ou/',\n",
              "  'lang': 'en',\n",
              "  'claim': 'The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"',\n",
              "  'doc': 'President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.',\n",
              "  'taxonomy_label': 'statistical',\n",
              "  'label_original': 'true'})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"raw_data/val_claims_quantemp.json\") as f:\n",
        "  val_data = json.load(f)\n",
        "len(val_data), val_data[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3360de2-689f-4c00-a806-bf74822f58ab",
      "metadata": {
        "id": "e3360de2-689f-4c00-a806-bf74822f58ab"
      },
      "source": [
        "# Extract Claims, Evidence, & Labels from data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef0f2b2-d1e2-4593-8944-3bdf62462a98",
      "metadata": {
        "id": "4ef0f2b2-d1e2-4593-8944-3bdf62462a98"
      },
      "outputs": [],
      "source": [
        "LE = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6f2a79-8684-48d3-9c1d-a8b1416359a5",
      "metadata": {
        "id": "9d6f2a79-8684-48d3-9c1d-a8b1416359a5"
      },
      "outputs": [],
      "source": [
        "def get_features(data):\n",
        "  features = []\n",
        "  evidences = []\n",
        "\n",
        "  for index, fact in enumerate(data):\n",
        "    claim = fact[\"claim\"]\n",
        "\n",
        "\n",
        "    feature = \"[Claim]:\"+claim+\"\\n[Evidences]:\"+fact[\"doc\"]\n",
        "    features.append(feature)\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c27f450-9982-431f-9aed-cbae5c22610c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c27f450-9982-431f-9aed-cbae5c22610c",
        "outputId": "5acd0dfb-7d5d-4d2a-e1a4-b2bf1e54a530"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9935,\n",
              " '[Claim]:FAKE:  Commandos from &#8220;Berkut&#8221; who refused to kneel have been burned alive in Lviv\\n[Evidences]:The Russian TV channel “Russia 1” aired a program called “Evil spirits of Maydan: mystic of Ukrainian mayhem”. The program, among other things, referred to the claim that two soldiers of “Berkut”, who refused to kneel in front of Lviv Maydan and recognize the current government, allegedly were burned alive. https://www.youtube.com/watch?v=SUDH0Qbjuao This was reported by the head of the so-called Russian community of Dnepropetrovsk Victor Trukhov. He says, two “Berkut” solders were put on their knees publicly and then burned in Lviv. However, contrary to this claim, a fire occured in Lviv on February 20, 2014 where people from security forces were caught in a fire. The fire started after a powerful explosion in the security forces basis, after which one officer in uniform and one in civilian clothes were pulled from the rubble. Commandos from “Berkut” kneeled on 24 February. Lviv citizens who came to the Maydan, were shouting “Shame!” and throwing small objects at security forces. To prevent any possible violence against “Berkut” soldiers, Self-Defense soldiers surrounded “Berkut”, and the priest was calming down people. \\xa0 People made the security forces to kneel on stage, and then one of the special forces soldier promised that “Berkut” will always be on the side of the people and assured that Lviv “Berkut” was not involved in the fighting in Kyiv. After the “ceremony” the commandos were taken away in the tram. Accordingly, the fire was the result of an accident and not the result of public humiliation of “Berkut”.')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features = get_features(train_data)\n",
        "len(train_features), train_features[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00dbb1e-890b-4568-a644-8af2eebd94cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d00dbb1e-890b-4568-a644-8af2eebd94cb",
        "outputId": "3265731e-7433-4082-d0da-1a2e05208d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3084,\n",
              " '[Claim]:The Biden administration \"published a study concluding 4 (of) 5 new cars on the road by 2050 will still require liquid fuels.\"\\n[Evidences]:President Joe Biden was in Michigan’s auto industry hub on Oct. 5 when he said, \"the whole world knows that the future of the auto industry is electric.\" Rep. Randy Feenstra, R-Iowa, had a quick response, writing on Twitter: \".@POTUS no it’s not — in fact, your own administration published a study concluding 4/5 new cars on the road by 2050 will still require liquid fuels ... \"It’s past time Biden lives up to his promise to expand clean-burning #biofuels. Don’t mess with the RFS!\" Feenstra is correct about the share of cars in the United States projected to use liquid fuels. The U.S. Energy Information Administration’s 2021 Annual Energy Outlook report, which projects the nation’s environmental plans through 2050, says about 79% of new vehicle sales will be powered by liquid fuels — gasoline and blends that include up to 85% ethanol — in 2050. They accounted for 95% of sales in 2020, the report states. PolitiFact contacted Feenstra’s communications staff over 10 times for comment but did not receive a response. However, Feenstra’s tweet shared the link to his sourcing, and on page 15, the federal report notes that, while electric energy’s biggest demand growth area is transportation, the share will be small — less than 3%: \"Current laws and regulations are not projected to induce much market growth, despite continuing improvements in electric vehicles (EVs) through evolutionary market developments. Both vehicle sales and utilization (miles driven) would need to increase substantially for EVs to raise electric power demand growth rates by more than a fraction of a percentage point per year.\" And, on the report’s page 26: \"Because most light-duty vehicles have internal combustion engines, motor gasoline remains the major transportation fuel through 2050 as personal travel returns to pre-pandemic per-driver levels in the longer term.\" While 2050 is 29 years down the road, Biden did not state in Michigan a time limit on when he thinks electric power would come close to liquid fuels’ share of the automobile market. Nor did he state in detail what that future for electric power for automobiles would be. His administration’s Annual Energy Outlook reports that it is not predicting future energy use; rather it is a projection based on assumptions and methodologies that can be changed when subjected to changes in technology, demographics and resources. The report also notes that it is a response to the Department of Energy Organization Act of 1977. That law requires the U.S. Energy Information Administration to produce annual reports on trends and projections for energy use and supply, the report notes. But Feenstra was precise with his facts, and is not alone reminding Biden to support biofuels. Other Iowa congressional members, Republican and Democrat, have pushed for biofuels support, regardless of who is president. So has the biofuels industry. The reason? Iowa leads the nation when it comes to producing biofuels from farm crops. It has capacity to produce 4.6 billion gallons, double the 2.3 billion in the No. 2 state, Nebraska. While we focused this story on predicted biofuels use in vehicles, it’s worth noting that Biden promised support for biofuels in a Sept. 9, 2020, campaign statement that criticized a pre-election policy reversal by then-President Donald Trump on biofuels. Trump’s reversal led to his administration’s rejection of oil refinery industry requests to be exempt from requirements to blend certain amounts of ethanol and biodiesel in gasoline: \"A Biden-Harris Administration will fight for family farmers and revitalize rural economies — from keeping our promises to farmers by ushering in a new era of biofuels, to investing in the broadband infrastructure and rural health care access that families and communities need,\" the Biden statement read. Trump reversed course again and approved on Jan. 19, 2021, just before leaving office, three biofuel waivers for refineries. Feenstra said the Biden Administration predicts that four out of five new cars on the road will require liquid fuels in 2050 and he cites the source. That source, a report from the Biden Administration, states that 79% of vehicles on the road will require liquid fuels by 2050. We rate Feenstra’s statement to be True.')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_features = get_features(val_data)\n",
        "len(val_features), val_features[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa12aa3-e8e8-4736-8cbd-fac304fbfbc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aa12aa3-e8e8-4736-8cbd-fac304fbfbc8",
        "outputId": "8ce4a834-cbeb-416b-b88c-1785755f6c3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9935, 'False', 3084, 'True')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels = [fact[\"label\"] for fact in train_data]\n",
        "val_labels = [fact[\"label\"] for fact in val_data]\n",
        "len(train_labels), train_labels[-1], len(val_labels), val_labels[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0634547d-05af-426d-82b8-fe32deab851f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0634547d-05af-426d-82b8-fe32deab851f",
        "outputId": "93129195-a908-4472-fdf2-b3c097b9d172"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9935,\n",
              " array([1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 0],\n",
              "       dtype=int64))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels_final = LE.fit_transform(train_labels)\n",
        "len(train_labels_final), train_labels_final[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56e5372-8424-4de6-a183-2ce5d0f438ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56e5372-8424-4de6-a183-2ce5d0f438ec",
        "outputId": "24a1e28e-da88-4a66-e493-a087d3c6120a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3084, array([1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1]))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_labels_final = LE.transform(val_labels)\n",
        "len(val_labels_final), val_labels_final[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3426eca-17ee-402f-90bb-d66dde83bc76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3426eca-17ee-402f-90bb-d66dde83bc76",
        "outputId": "791fe1ec-6768-47d3-aaa8-0b191f2724ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([9935]), torch.Size([3084]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels_final = torch.tensor(train_labels_final)\n",
        "val_labels_final = torch.tensor(val_labels_final)\n",
        "train_labels_final.shape, val_labels_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1904cc-c121-41c2-bb60-6ce3bac0122e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e1904cc-c121-41c2-bb60-6ce3bac0122e",
        "outputId": "312e59f6-57f3-4d2a-9a0b-de50b8210110"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, ['True', 'Conflicting', 'False'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(list(set(train_labels)))\n",
        "num_classes, list(set(train_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f35310f-c79a-47f8-94cb-70dc78317ee2",
      "metadata": {
        "id": "1f35310f-c79a-47f8-94cb-70dc78317ee2"
      },
      "source": [
        "# Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7f709b-dc29-4181-b9e6-17c8af93c705",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa7f709b-dc29-4181-b9e6-17c8af93c705",
        "outputId": "2c0959e7-1486-453b-d5a9-6b695ec142e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sowmy\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  [Claim]:In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country.\n",
            "[Evidences]:Did Finance Minister Nirmala Sitharaman claim the government distributed 35,000 crore LED bulbs under the Ujala scheme? This would imply the Modi govt gave about 300 bulbs to every person in India. At least this is what is being claimed by some social media users who are sharing a screenshot from a news segment on business channel CNBC Awaaz. The photo shows Sitharaman delivering her budget speech while a caption at the bottom reads - \"35,000 crore LED bulb baantein gaye\" (35,000 crore LED bulbs were distributed). The snapshot gives the impression that Sitharaman said this sentence in her speech. Netizens are displaying shock at this whopping number believing that the finance minister's statement is true. Some Congress leaders are also trolling her by sharing the screenshot of the news channel. But, India Today Anti Fake News War Room (AFWA) found that Sitharaman never said that 35,000 crore LED bulbs were distributed in the country in her speech. In fact, she said that approximately 35 crore LED bulbs were distributed under the Ujala scheme. Among many who have shared the news channel's screenshot is Congress spokesperson Shobha Oza who tweeted the picture. Her tweet was retweeted more than 350 times and had over 1,500 likes by the time of writing this story. Former Cabinet Minister in Haryana government, Mahender Pratap Singh also trolled Sitharaman, believing the news to be true. There are some more verified social media users who have shared the same image of the news channel. 125 35000 LED - 280 .. , ...? pic.twitter.com/p8JKRWrMPb Sachin Chaudhary (@SChaudharyINC) July 6, 2019 Every Indian after receiving 300 LED bulbs each, as claimed by Dumb Sitharaman !! (She claims 35000 crore LED bulbs have been distributed, which means every Indian must have got approx 300 each) pic.twitter.com/7Nsz0ZYLm6 Gaurav Pandhi (@GauravPandhi) July 6, 2019 At 1:06:47, in the YouTube video of the Budget 2019 speech, one can hear Finance Minister Nirmala Sitharaman saying \"approximately 35 crore LEDs have been distributed under Ujala Yojana\". The text speech of the budget also reads \"approximately 35 crore LED bulbs\", and not \"35,000 crore LEDs\". Website of UJALA also states that till July 6, more than 35 crore LEDs have been distributed in the country. A senior editor at CNBC Awaaz spoke to us and confirmed that the wrong figure was aired due to a typo which was corrected when noticed. INDIA TODAY FACT CHECK Claim In her budget speech, Nirmala Sitharaman claimed that the Government distributed 35,000 crore LED bulbs in the country. Conclusion Sitharaman never said this in the budget speech. She stated that about 35 crore LED bulbs were distributed under UJALA scheme. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n",
            "Token IDs: tensor([     1,    647,  71216,    592,    294,   2514,    342,   1753,   2890,\n",
            "           261, 114991,  15461,  18521,   1246,   3484,    272,    262,   2266,\n",
            "          4928,   2453,    261,    528,  12388,   4546,  10191,    267,    262,\n",
            "           658,    260,    647, 101860,    268,    592,    294,  20788,   5442,\n",
            "          2357, 114991,  15461,  18521,   1246,   1674,    262,    671,   4928,\n",
            "          2453,    261,    528,  12388,   4546,  10191,    494,    262,    543,\n",
            "         81976,   3630,    302,    329,    338,  12683,    262,  11870,  31025,\n",
            "          1255,    314,   3047,  10191,    264,    469,    604,    267,   1280,\n",
            "           260,    620,    668,    291,    269,    339,    269,    411,   3484,\n",
            "           293,    347,    728,    806,   1133,    328,    281,   1881,    266,\n",
            "         16697,    292,    266,    984,   4849,    277,    460,   3024,  29186,\n",
            "           336,   6608,  12113,    260,    279,   1456,   1057,  15461,  18521,\n",
            "          1246,   4788,    342,   1753,   2890,    438,    266,  11743,    288,\n",
            "           262,   1741,   8036,    341,    307,   3377,    261,    528,  12388,\n",
            "          4546,  11738,  17601,  13618,    547,   4733,    473,    309,    287,\n",
            "          3377,    261,    528,  12388,   4546,  10191,    332,   4928,    285,\n",
            "           260,    279,  16013,   1360,    262,   5035,    272,  15461,  18521,\n",
            "          1246,    357,    291,   4378,    267,    342,   2890,    260,   5885,\n",
            "         45445,    268,    281,  10475,   5137,    288,    291,  18931,    496,\n",
            "          9581,    272,    262,   3769,   3931,    280,    268,   1548,    269,\n",
            "           980,    260,    879,   2556,   1840,    281,    327,  32150,    342,\n",
            "           293,   1881,    262,  16697,    265,    262,    984,   3024,    260,\n",
            "           420,    261,   1280,   1715,   5904,  22875,   1864,   1752,   3515,\n",
            "           287,  18514,  14862,    285,    505,    272,  15461,  18521,   1246,\n",
            "           518,    357,    272,   2453,    261,    528,  12388,   4546,  10191,\n",
            "           332,   4928,    267,    262,    658,    267,    342,   2890,    260,\n",
            "           344,    713,    261,    373,    357,    272,   2407,   2453,  12388,\n",
            "          4546,  10191,    332,   4928,    494,    262,    543,  81976,   3630,\n",
            "           260,   4544,    386,      2])\n"
          ]
        }
      ],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in train_features:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', train_features[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b69df8-e154-4db6-9f2c-47964ab2fb68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b69df8-e154-4db6-9f2c-47964ab2fb68",
        "outputId": "e857a14f-5590-478a-9228-f2fc38960685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  [Claim]:Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor.\n",
            "[Evidences]:The India Today Anti-Fake News War Room found the viral video of Amit Shah's statement was clipped and presented out of context. A short video clip of Union Home Minister Amit Shah has gone viral with the claim that at a political rally, he said that Prime Minister Narendra Modi sleeps 24 hours for the welfare of the poor. Several Twitter and Facebook users shared this video clip with captions like, “Modi ji sleeps for 24 hours”. The India Today Anti-Fake News War Room ( AFWA) found the viral video was clipped and presented out of context to give it a different meaning. In the original video, Shah can be heard saying that PM Modi thinks about the welfare of the poor 24 hours a day while “Didi” (Mamta Banerjee) wonders when her nephew would become the Chief Minister. Shah made the statement while addressing a public meeting in Chapra, West Bengal, in April 2021. The viral posts are archived here, here, and here. AFWA probe With the help of keywords searches, we discovered that the same video along with the same claim had made the rounds of social media in 2021 as well. We found the original video on the official Bharatiya Janata Party YouTube channel. It was uploaded on April 17, 2021. The title of the YouTube video read, “HM Shri Amit Shah addresses public meeting in Chapra, West Bengal.” At the 5.34-minutes mark, one can hear the unedited version of the portion of the speech that went viral. In the original video, Shah was talking about the Trinamool Congress-led government in West Bengal. Shah said “This is a government for the welfare of the nephew... Modi ji thinks about the welfare of the poor, 24 hours a day. And Didi thinks about when her nephew will become Chief Minister, 24 hours a day.” Instead of “sote” (sleep), Shah said “sochte” (thinks) in the original video. But the video was clipped to change the meaning of the sentence altogether. Thus, an old clipped video of Amit Shah has gone viral with a misleading claim. (With inputs from Sanjana Saxena) INDIA TODAY FACT CHECK Claim Amit Shah said Narendra Modi sleeps for 24 hours for the welfare of the poor. Conclusion The viral video is clipped. In the original video, Shah says PM Modi thinks about the welfare of the poor, 24 hours a day, while “Didi” (Mamata Bannerjee) wonders when her nephew would become CM. Shah was addressing a public meeting in Chapra, West Bengal, in April 2021. JHOOTH BOLE KAUVA KAATE The number of crows determines the intensity of the lie. 1 Crow: Half True 2 Crows: Mostly lies 3 Crows: Absolutely false\n",
            "Token IDs: tensor([     1,    647,  71216,    592,    294,  81211,    297,  11209,    357,\n",
            "         24219,  11870,  24949,    270,    969,    743,    270,    262,   6714,\n",
            "           265,    262,   1970,    260,    647, 101860,    268,    592,    294,\n",
            "           635,   1280,   1715,   5904,    271,  74919,   1864,   1752,   3515,\n",
            "           505,    262,   8804,    750,    265,  35741,  11209,    280,    268,\n",
            "          1548,    284,  32140,    263,   1992,    321,    265,   2758,    260,\n",
            "           336,    782,    750,   5981,    265,   2432,   1393,   2357,  35741,\n",
            "         11209,    303,   1699,   8804,    275,    262,   1674,    272,    288,\n",
            "           266,   1198,   6917,    261,    313,    357,    272,   3518,   2357,\n",
            "         24219,  11870,  24949,    969,    743,    270,    262,   6714,    265,\n",
            "           262,   1970,    260,   4697,   2125,    263,   1401,   1133,   1848,\n",
            "           291,    750,   5981,    275,  35798,    334,    261,    317,  34082,\n",
            "           667,  26196,  24949,    270,    969,    743,    318,    260,    279,\n",
            "          1280,   1715,   5904,    271,  74919,   1864,   1752,   3515,    287,\n",
            "         14439,  14862,    285,    505,    262,   8804,    750,    284,  32140,\n",
            "           263,   1992,    321,    265,   2758,    264,    527,    278,    266,\n",
            "           467,   2094,    260,    344,    262,   1020,    750,    261,  11209,\n",
            "           295,    282,   1331,   1266,    272,   2839,  11870,   5470,    314,\n",
            "           262,   6714,    265,    262,   1970,    969,    743,    266,    406,\n",
            "           438,    317,  20788,    667,    318,    287,   1135,   1764,   3321,\n",
            "         55849,    285,  10120,    335,    342,  15876,    338,    638,    262,\n",
            "          3234,   2357,    260,  11209,    412,    262,   1548,    438,   6910,\n",
            "           266,    613,   1122,    267,  29607,   3608,    261,   1260,  17906,\n",
            "           261,    267,   1217,  16789,    260,    279,   8804,   2185,    281,\n",
            "         23848,    422,    261,    422,    261,    263,    422,    260,  14439,\n",
            "         14862,   8704,    559,    262,    408,    265,   8451,   8786,    261,\n",
            "           301,   2534,    272,    262,    454,    750,    641,    275,    262,\n",
            "           454,   1674,    330,    412,    262,   6958,    265,    728,    806,\n",
            "           267,  16789,    283,      2])\n"
          ]
        }
      ],
      "source": [
        "  val_input_ids = []\n",
        "  val_attention_masks = []\n",
        "\n",
        "  for sent in val_features:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 256,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          truncation=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "\n",
        "      # Add the encoded sentence to the list.\n",
        "      val_input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      val_attention_masks.append(encoded_dict['attention_mask'])\n",
        "  # Convert the lists into tensors.\n",
        "  val_input_ids = torch.cat(val_input_ids, dim=0)\n",
        "  val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
        "\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', val_features[0])\n",
        "  print('Token IDs:', val_input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3a6e2d-f32f-4944-9e3f-e5cd4760d2cf",
      "metadata": {
        "id": "3f3a6e2d-f32f-4944-9e3f-e5cd4760d2cf"
      },
      "source": [
        "# Create the DataLoaders from tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff66c81-810e-4bdf-8bad-527d370a5663",
      "metadata": {
        "id": "bff66c81-810e-4bdf-8bad-527d370a5663"
      },
      "outputs": [],
      "source": [
        "# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
        "# difficulty_tensor = torch.tensor(difficulty_level_vectors,dtype=torch.float)\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, train_labels_final)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf8e61d-d624-4a80-9a7f-d39cb13a04a0",
      "metadata": {
        "id": "fdf8e61d-d624-4a80-9a7f-d39cb13a04a0"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = RandomSampler(dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f24d56-d4ef-419d-834f-bfdc46da5680",
      "metadata": {
        "id": "65f24d56-d4ef-419d-834f-bfdc46da5680"
      },
      "source": [
        "# Set up the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7b33b4-92e4-4a07-b87c-453aaefe526b",
      "metadata": {
        "id": "4c7b33b4-92e4-4a07-b87c-453aaefe526b"
      },
      "outputs": [],
      "source": [
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_path, labels_count, hidden_dim=768, mlp_dim=500, extras_dim=100, dropout=0.1, freeze_bert=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dberta = AutoModel.from_pretrained(bert_model_path, output_hidden_states=True, output_attentions=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(mlp_dim, mlp_dim),\n",
        "            # # nn.ReLU(),\n",
        "            # # nn.Linear(mlp_dim, mlp_dim),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, labels_count)\n",
        "        )\n",
        "        # self.softmax = nn.LogSoftmax(dim=1)\n",
        "        if freeze_bert:\n",
        "            print(\"Freezing layers\")\n",
        "            for param in self.dberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, tokens, masks):\n",
        "        output = self.dberta(tokens, attention_mask=masks)\n",
        "        #dropout_output = self.dropout(output[\"pooler_output\"])\n",
        "        cls_output = output[\"last_hidden_state\"][:, 0, :]\n",
        "        # concat_output = torch.cat((dropout_output, topic_emb), dim=1)\n",
        "        # concat_output = self.dropout(concat_output)\n",
        "        mlp_output = self.mlp(cls_output)\n",
        "        # proba = self.sigmoid(mlp_output)\n",
        "        # proba = self.softmax(mlp_output)\n",
        "\n",
        "        return mlp_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fbed426-d306-461d-bbb5-1415d136ce01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900,
          "referenced_widgets": [
            "292ce68c3bb5445bbed06de4ed50882e",
            "2fdbea507068401dad40fc8f685540f0",
            "13069793c1f94805ba7794a129d5e139",
            "4c782f08e72740279aee19c69e1a0615",
            "42e358b3587346f7a017d910a08f4bf4",
            "25640afd460845ed85314598a3ff6db5",
            "1dc4a6f669884f828e7d636cc3764b01",
            "c8ab6228f8214f07856df5e678504745",
            "768bb6a8d1db40ddb02de2c614103a25",
            "76dc372636684b7fac56c23e8b330135",
            "a2ae311912f04ef3b66a7580ae300c5b"
          ]
        },
        "id": "2fbed426-d306-461d-bbb5-1415d136ce01",
        "outputId": "a76bb35c-5fd4-4a94-dc9b-8f26bd746326"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiClassClassifier(\n",
              "  (dberta): DebertaV2Model(\n",
              "    (embeddings): DebertaV2Embeddings(\n",
              "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaV2Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DebertaV2Layer(\n",
              "          (attention): DebertaV2Attention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaV2SelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaV2Intermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaV2Output(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=768, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single\n",
        "model = MultiClassClassifier('sileod/deberta-v3-base-tasksource-nli',num_classes, 1024, 768, 140, dropout=0.1, freeze_bert=False)\n",
        "\n",
        "# model.load_state_dict(torch.load(\"model_bert_difficulty_prediction/model_weights\"))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c5cd4d-430d-4579-9b04-c3873ac60b1e",
      "metadata": {
        "id": "92c5cd4d-430d-4579-9b04-c3873ac60b1e"
      },
      "source": [
        "# Set up the training optimizer and learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934bf4cb-419f-40d8-95f1-470b10867d1d",
      "metadata": {
        "id": "934bf4cb-419f-40d8-95f1-470b10867d1d"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76adf61c-e105-433e-95c5-a8207f66819d",
      "metadata": {
        "id": "76adf61c-e105-433e-95c5-a8207f66819d"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5d9668-c163-42c4-894a-a749b35283ca",
      "metadata": {
        "id": "9d5d9668-c163-42c4-894a-a749b35283ca"
      },
      "source": [
        "# Set up training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a292bf4f-9616-4ebe-9f5e-06b55e1868c5",
      "metadata": {
        "id": "a292bf4f-9616-4ebe-9f5e-06b55e1868c5"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4349908-450e-46eb-99a2-c316de9beedf",
      "metadata": {
        "id": "b4349908-450e-46eb-99a2-c316de9beedf"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b62ec49-b8da-4645-87f2-d6427c150767",
      "metadata": {
        "id": "4b62ec49-b8da-4645-87f2-d6427c150767"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5334d99b-52d6-4b9c-9716-867fd0db1b1b",
      "metadata": {
        "id": "5334d99b-52d6-4b9c-9716-867fd0db1b1b"
      },
      "outputs": [],
      "source": [
        "for param in model.dberta.encoder.layer[0:5].parameters():\n",
        "    param.requires_grad=False # freeze the first 5 layers of the roberta model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c88add-5d7e-47fd-a29b-cccf89f5d4f3",
      "metadata": {
        "id": "06c88add-5d7e-47fd-a29b-cccf89f5d4f3"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed6255d-6003-4199-ad9e-439aba096d8b",
      "metadata": {
        "id": "bed6255d-6003-4199-ad9e-439aba096d8b"
      },
      "source": [
        "# Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fa471c-d3c0-4ef6-ad2b-25dd99a20846",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5fa471c-d3c0-4ef6-ad2b-25dd99a20846",
        "outputId": "8707b085-e4c2-4fd2-d9e8-69689b4a70cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 80.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[51], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Perform a forward pass (evaluate the model on this training batch).\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m probas \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Accumulate the training loss over all of the batches so that we can\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# calculate the average loss at the end. `loss` is a Tensor containing a\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# single value; the `.item()` function just returns the Python value\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# from the tensor.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(probas, b_labels)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[42], line 23\u001b[0m, in \u001b[0;36mMultiClassClassifier.forward\u001b[1;34m(self, tokens, masks)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, masks):\n\u001b[1;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#dropout_output = self.dropout(output[\"pooler_output\"])\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     cls_output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1061\u001b[0m )\n\u001b[1;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    499\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m         output_attentions,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    348\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m ):\n\u001b[1;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    285\u001b[0m ):\n\u001b[1;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:714\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[0;32m    713\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[1;32m--> 714\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:786\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc2p\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_att_type:\n\u001b[0;32m    785\u001b[0m     scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(pos_key_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m*\u001b[39m scale_factor)\n\u001b[1;32m--> 786\u001b[0m     c2p_att \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_key_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    787\u001b[0m     c2p_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(relative_pos \u001b[38;5;241m+\u001b[39m att_span, \u001b[38;5;241m0\u001b[39m, att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    788\u001b[0m     c2p_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    789\u001b[0m         c2p_att,\n\u001b[0;32m    790\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    791\u001b[0m         index\u001b[38;5;241m=\u001b[39mc2p_pos\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand([query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), relative_pos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]),\n\u001b[0;32m    792\u001b[0m     )\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 80.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "early_stopping = EarlyStopping(patience=2, verbose=True, path=\"checkpoint_roberta_large_mnli.pt\")\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_accuracy = 0\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        # b_poincare = batch[2].to(device)\n",
        "        # b_difficulty = batch[3].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # skill_labels = batch[3].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        probas = model(b_input_ids,b_input_mask)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        loss = loss_func(probas, b_labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        # scheduler.step()\n",
        "        logits = probas.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
        "    print(\" Train Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        # b_poincare = batch[2].to(device)\n",
        "        # b_difficulty = batch[3].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # skill_labels = batch[3].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "\n",
        "          logits = model(b_input_ids,b_input_mask)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        loss = loss_func(logits, b_labels)\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    early_stopping(avg_val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    output_dir = 'deberta-v3-base-tasksource-nli/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
        "\n",
        "    # !rm -rf \"/content/drive/My Drive/DSAIT4090_FinalProject/code/roberta_large_mnli/roberta_large_mnli\"\n",
        "    # !mv roberta_large_mnli \"/content/drive/My Drive/DSAIT4090_FinalProject/code/roberta_large_mnli\"\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38IPHp0bLrxf",
      "metadata": {
        "id": "38IPHp0bLrxf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13069793c1f94805ba7794a129d5e139": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8ab6228f8214f07856df5e678504745",
            "max": 1425698116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_768bb6a8d1db40ddb02de2c614103a25",
            "value": 1425698116
          }
        },
        "1dc4a6f669884f828e7d636cc3764b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25640afd460845ed85314598a3ff6db5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292ce68c3bb5445bbed06de4ed50882e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fdbea507068401dad40fc8f685540f0",
              "IPY_MODEL_13069793c1f94805ba7794a129d5e139",
              "IPY_MODEL_4c782f08e72740279aee19c69e1a0615"
            ],
            "layout": "IPY_MODEL_42e358b3587346f7a017d910a08f4bf4"
          }
        },
        "2fdbea507068401dad40fc8f685540f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25640afd460845ed85314598a3ff6db5",
            "placeholder": "​",
            "style": "IPY_MODEL_1dc4a6f669884f828e7d636cc3764b01",
            "value": "model.safetensors: 100%"
          }
        },
        "42e358b3587346f7a017d910a08f4bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c782f08e72740279aee19c69e1a0615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dc372636684b7fac56c23e8b330135",
            "placeholder": "​",
            "style": "IPY_MODEL_a2ae311912f04ef3b66a7580ae300c5b",
            "value": " 1.43G/1.43G [00:08&lt;00:00, 227MB/s]"
          }
        },
        "768bb6a8d1db40ddb02de2c614103a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76dc372636684b7fac56c23e8b330135": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ae311912f04ef3b66a7580ae300c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ab6228f8214f07856df5e678504745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}