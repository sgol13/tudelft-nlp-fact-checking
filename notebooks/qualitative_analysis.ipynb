{
 "cells": [
  {
   "cell_type": "code",
   "id": "1db19c7c-a2df-4e00-8bd7-0469f2aca7dc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.transform import cumsum, linear_cmap\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models import ColumnDataSource, ColorBar\n",
    "#from bokeh.palettes import RdYlGn as colors\n",
    "from bokeh.palettes import Plasma256 as colors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook() # activate Bokeh output to Jupyter notebook"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c25087ea-1e56-40dc-9e89-6b781b2780de",
   "metadata": {},
   "source": [
    "with open(\"../data/raw_data/test_claims_quantemp.json\") as f:\n",
    "    test_json = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f27d87-a1c9-4a92-9c34-0cd58796a29b",
   "metadata": {},
   "source": [
    "with open(\"test_evidences_decomposed_custom.json\") as f:\n",
    "    test_json_custom = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c5ae3ce-38e1-4e91-8773-4af5ac4d258d",
   "metadata": {},
   "source": [
    "len(test_json)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc063419-ace8-43c3-b38a-5ae7cb62bda9",
   "metadata": {},
   "source": [
    "len(test_json_custom)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4505da74-c6d3-4576-8989-4540e5ba29fc",
   "metadata": {},
   "source": [
    "# For gold standard figures only use this csvs_to_test\n",
    "# csvs_to_test = {\n",
    "#     \"RoBERTa-MNLI:Gold\": \"roberta_large_mnli.csv\",\n",
    "#     \"BART-MNLI:Gold\": \"bart_large_mnli.csv\",\n",
    "#     \"DeBERTa-NLI:Gold\": \"deberta-v3-base-tasksource-nli_claimdecomp.csv\",\n",
    "#     \"Flan-T5:Gold\": \"flan-t5-base-eval.csv\",\n",
    "#     \"Math-RoBERTa:Gold\": \"math_roberta.csv\",\n",
    "#     \"Numeric-T5:Gold\": \"numt5.csv\",\n",
    "#     \"PASTA:Gold\": \"pasta-nli_claimdecomp_output.csv\",\n",
    "#     \"Elastic-RoBERTa:Gold\": \"elastic_roberta_claimdecomp_output2.csv\",\n",
    "# }  \n",
    "\n",
    "# For decomp figures use this csvs_to_test\n",
    "csvs_to_test = {\n",
    "    #\"GPT-2:Gold\": \"gpt2_doc.csv\",\n",
    "    \"RoBERTa-MNLI:Gold\": \"roberta_large_mnli.csv\",\n",
    "    \"RoBERTa-MNLI:OrigClaim\": \"roberta_large_mnli_no_decomposition.csv\",\n",
    "    \"RoBERTa-MNLI:CLAIMDECOMP\": \"roberta_large_mnli_flant5.csv\",\n",
    "    \"RoBERTa-MNLI:Pseudo Program-FC\": \"roberta_large_mnli_gpt3.5-turbo.csv\",\n",
    "    \"RoBERTa-MNLI:Custom\": \"roberta_large_mnli_custom.csv\",\n",
    "    \"BART-MNLI:Gold\": \"bart_large_mnli.csv\",\n",
    "    \"BART-MNLI:OrigClaim\": \"bartMNLI_origclaim.csv\",\n",
    "    \"BART-MNLI:CLAIMDECOMP\": \"bartMNLI_decomp_flant5.csv\",\n",
    "    \"BART-MNLI:Pseudo Program-FC\": \"bart_large_mnli_gpt3.5-turbo.csv\",\n",
    "    \"BART-MNLI:Custom\": \"bart_large_mnli_custom.csv\",\n",
    "    \"DeBERTa-NLI:Gold\": \"deberta-v3-base-tasksource-nli_claimdecomp.csv\",\n",
    "    \"DeBERTa-NLI:OrigClaim\": \"deberta_v3_no_decomposition.csv\",\n",
    "    \"DeBERTa-NLI:CLAIMDECOMP\": \"deberta_v3_flant5.csv\",\n",
    "    \"DeBERTa-NLI:Pseudo Program-FC\": \"deberta_v3_gpt3.5-turbo.csv\",\n",
    "    \"DeBERTa-NLI:Custom\": \"deberta_v3_custom.csv\",\n",
    "    \"Flan-T5:Gold\": \"flan-t5-base-eval.csv\",\n",
    "    \"Flan-T5:OrigClaim\": \"flant5_no_decomposition.csv\",\n",
    "    \"Flan-T5:CLAIMDECOMP\": \"flant5_flant5.csv\",\n",
    "    \"Flan-T5:Pseudo Program-FC\": \"flant5_gpt3.5-turbo.csv\",\n",
    "    \"Flan-T5:Custom\": \"flant5_custom.csv\",\n",
    "    \n",
    "    \"Math-RoBERTa:Gold\": \"math_roberta.csv\",\n",
    "    \"Math-RoBERTa:OrigClaim\": \"math_roberta_no_decomposition.csv\",\n",
    "    \"Math-RoBERTa:CLAIMDECOMP\": \"math_roberta_flant5.csv\",\n",
    "    \"Math-RoBERTa:Pseudo Program-FC\": \"math_roberta_gpt3.5-turbo.csv\",\n",
    "    \"Math-RoBERTa:Custom\": \"math_roberta_custom.csv\",\n",
    "    \"Numeric-T5:Gold\": \"numt5.csv\",\n",
    "    \"Numeric-T5:OrigClaim\": \"numt5_no_decomposition.csv\",\n",
    "    \"Numeric-T5:CLAIMDECOMP\": \"numt5_flant5.csv\",\n",
    "    \"Numeric-T5:Pseudo Program-FC\": \"numt5_gpt3.5-turbo.csv\",\n",
    "    \"Numeric-T5:Custom\": \"numt5_custom.csv\",\n",
    "    \"PASTA:Gold\": \"pasta-nli_claimdecomp_output.csv\",\n",
    "    \"PASTA:OrigClaim\": \"pasta_no_decomposition.csv\",\n",
    "    \"PASTA:CLAIMDECOMP\": \"pasta_flant5.csv\",\n",
    "    \"PASTA:Pseudo Program-FC\": \"pasta_gpt3.5-turbo.csv\",\n",
    "    \"PASTA:Custom\": \"pasta_custom.csv\",\n",
    "    \"Elastic-RoBERTa:Gold\": \"elastic_roberta_claimdecomp_output2.csv\",\n",
    "    \"Elastic-RoBERTa:OrigClaim\": \"elasticRoBERTa_origclaim.csv\",\n",
    "    \"Elastic-RoBERTa:CLAIMDECOMP\": \"elasticRoBERTa_decomp_flant5.csv\",\n",
    "    \"Elastic-RoBERTa:Pseudo Program-FC\": \"elastic_roberta_gpt3.5-turbo.csv\",\n",
    "    \"Elastic-RoBERTa:Custom\": \"elastic_roberta_custom.csv\",\n",
    "}  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe76eb08-a566-4a7e-a83d-027984f92076",
   "metadata": {},
   "source": [
    "# Pies: What the prediction distribution looked like for each model"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ed8c1b2-e6f8-4e22-9c6e-2c9aafc6a62e",
   "metadata": {},
   "source": [
    "value_counts = {\"Ground Truth\": pd.Series([sample['label'] for sample in test_json]).value_counts().sort_index(ascending=False).to_dict()}\n",
    "for key in csvs_to_test:\n",
    "    if not csvs_to_test[key]:\n",
    "        continue\n",
    "    df = pd.read_csv(csvs_to_test[key])\n",
    "\n",
    "    \n",
    "    if key.split(\":\")[-1] == \"Custom\" or key == \"PASTA:CLAIMDECOMP\":\n",
    "        for tup, test_sample in zip(df.itertuples(), test_json_custom):\n",
    "            assert tup.claim == test_sample['claim'], (key, tup.claim, test_sample['claim'], ctr)\n",
    "    else:\n",
    "\n",
    "        for tup, test_sample in zip(df.itertuples(), test_json):\n",
    "            assert tup.claim == test_sample['claim'], (key, tup.claim, test_sample['claim'], ctr)\n",
    "\n",
    "    value_counts[key] = df['verdict'].value_counts().sort_index(ascending=False).to_dict()\n",
    "value_counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e38b6c3-220a-4482-8d86-a2a8d0caa572",
   "metadata": {},
   "source": [
    "# Helper function to create a pie chart\n",
    "def create_pie_chart(data_dict, title):\n",
    "    data = pd.Series(data_dict).reset_index(name='value').rename(columns={'index': 'label'})\n",
    "    data['angle'] = data['value']/data['value'].sum() * 2*pi\n",
    "    data['color'] = Category20c[len(data_dict)]\n",
    "\n",
    "    # Create a pie chart\n",
    "    p = figure(height=200, width=400, title=title, toolbar_location=\"left\", tools=\"hover,save,pan,box_zoom,reset\", tooltips=\"@label: @value\", x_range=(-1, 1))\n",
    "    \n",
    "    p.wedge(x=0, y=1, radius=0.4, \n",
    "            start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'), \n",
    "            line_color=\"white\", fill_color='color', legend_field='label', source=data)\n",
    "    \n",
    "    p.xgrid.visible = False\n",
    "    p.ygrid.visible = False\n",
    "    p.axis.visible = False\n",
    "\n",
    "    # p.wedge.text(x=0, y=1, angle=cumsum('angle', include_zero=True),\n",
    "    #              text='label', source=data, text_align='center')\n",
    "\n",
    "    return p\n",
    "\n",
    "# # Create pie charts for each of the dataframes\n",
    "plots = []\n",
    "for key in value_counts:\n",
    "    plot = create_pie_chart(value_counts[key], key)\n",
    "    plots.append(plot)\n",
    "plots.insert(1, None)\n",
    "plots.insert(1, None)\n",
    "plots.insert(3, None)\n",
    "plots.insert(3, None)\n",
    "\n",
    "# Arrange the pie charts in a grid layout (2 rows and 4 columns)\n",
    "grid = gridplot([plots[i:i+5] for i in range(0, len(plots), 5)])\n",
    "\n",
    "# # Output to an HTML file\n",
    "# output_file(\"pie_charts_grid.html\")\n",
    "show(grid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9dc060f3-3dce-486d-8986-2224c697b41e",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb4a60f0-f9ce-4d5f-9bd4-3280af603b75",
   "metadata": {},
   "source": [
    "def create_confusion_matrix(ground_truth, predicted_labels, title):  \n",
    "    # Compute the confusion matrix\n",
    "    labels = sorted(set(ground_truth + predicted_labels))  # Ensure labels are consistent across both lists\n",
    "    conf_matrix = confusion_matrix(ground_truth, predicted_labels, labels=labels)\n",
    "\n",
    "    conf_matrix_percent = conf_matrix.astype('float')  # Convert to float for division\n",
    "    row_sums = conf_matrix_percent.sum(axis=1)  # Sum of each row (ground truth label)\n",
    "    conf_matrix_percent = (conf_matrix_percent.T / row_sums).T * 100  # Normalize and convert to percentage\n",
    "    \n",
    "    # Create a DataFrame for easier manipulation\n",
    "    df_conf_matrix = pd.DataFrame(conf_matrix_percent, index=labels, columns=labels)\n",
    "    # Create a DataFrame for easier manipulation\n",
    "    #df_conf_matrix = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "    \n",
    "    # Prepare data for the Bokeh plot\n",
    "    x_range = labels \n",
    "    y_range = labels  # Reverse y-axis for better visualization (top-down)\n",
    "    values = df_conf_matrix.values.flatten()\n",
    "    \n",
    "    # Generate the ColumnDataSource for Bokeh\n",
    "    source = ColumnDataSource(data=dict(\n",
    "        x=np.tile(np.arange(len(labels)), len(labels)),\n",
    "        y=np.repeat(np.arange(len(labels)), len(labels)),\n",
    "        value=values\n",
    "    ))\n",
    "    \n",
    "    # Set up the color mapping\n",
    "    mapper = linear_cmap(field_name='value', palette=colors, low=0, high=100)\n",
    "    \n",
    "    # Create the figure for plotting\n",
    "    p = figure(title=title, x_range=x_range, y_range=y_range,\n",
    "               toolbar_location=None, tools=\"hover,save,pan,box_zoom,reset\", \n",
    "               tooltips=\"@x, @y: @value%\", width=300, height=300,\n",
    "               # x_range=(-0.5, len(labels) - 0.5),  # Add padding to the left and right\n",
    "               # y_range=(-0.5, len(labels) - 0.5),  # Add padding to the top and bottom\n",
    "              )\n",
    "    \n",
    "    # Create a heatmap\n",
    "    p.rect(x='x', y='y', width=1, height=1, source=source,\n",
    "           line_color=\"white\", fill_color=mapper)\n",
    "    \n",
    "    # Add color bar\n",
    "    color_bar = ColorBar(color_mapper=mapper['transform'], width=8, location=(0, 0))\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    # Customize axis labels\n",
    "    p.xaxis.axis_label = 'Predicted'\n",
    "    p.yaxis.axis_label = 'Ground Truth'\n",
    "    p.xaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "    return p"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f425f2c-1bbd-4bcb-9a66-c9cb78c9b4ae",
   "metadata": {},
   "source": [
    "ground_truth = [sample['label'] for sample in test_json]\n",
    "ground_truth_custom = [sample['label'] for sample in test_json_custom]\n",
    "predicted_labels = {\"Ground Truth\": ground_truth}\n",
    "for key in csvs_to_test:\n",
    "    if not csvs_to_test[key]:\n",
    "        continue\n",
    "    df = pd.read_csv(csvs_to_test[key])\n",
    "\n",
    "    if key.split(\":\")[-1] == \"Custom\" or key == \"PASTA:CLAIMDECOMP\":\n",
    "        for tup, test_sample in zip(df.itertuples(), test_json_custom):\n",
    "            assert tup.claim == test_sample['claim']\n",
    "    else:\n",
    "        for tup, test_sample in zip(df.itertuples(), test_json):\n",
    "            assert tup.claim == test_sample['claim']\n",
    "    \n",
    "    predicted_labels[key] = df['verdict'].to_list()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "010ee374-8d35-49ae-b00f-f7000086800e",
   "metadata": {},
   "source": [
    "# Create confusion matrices for each of the dataframes\n",
    "plots = []\n",
    "for key in predicted_labels:\n",
    "    if key.split(\":\")[-1] == \"Custom\" or key == \"PASTA:CLAIMDECOMP\":\n",
    "        plot = create_confusion_matrix(ground_truth_custom, predicted_labels[key], key)\n",
    "    else:\n",
    "        plot = create_confusion_matrix(ground_truth, predicted_labels[key], key)\n",
    "    plots.append(plot)\n",
    "plots.insert(1, None)\n",
    "plots.insert(1, None)\n",
    "plots.insert(3, None)\n",
    "plots.insert(3, None)\n",
    "\n",
    "# Arrange the pie charts in a grid layout (2 rows and 4 columns)\n",
    "grid = gridplot([plots[i:i+5] for i in range(0, len(plots), 5)])\n",
    "\n",
    "# # Output to an HTML file\n",
    "# output_file(\"pie_charts_grid.html\")\n",
    "show(grid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7c0cc2d-14ce-4b66-9d55-cc5cf53660eb",
   "metadata": {},
   "source": [
    "# Qualitative Analysis: Randomly sample some claims for each GOLD model"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5f9dd51-0d5a-47e9-8d4e-bfd62a2c9097",
   "metadata": {},
   "source": [
    "ground_truth = np.array([sample['label'] for sample in test_json])\n",
    "predicted_labels = dict()\n",
    "got_wrong = dict()\n",
    "for key in csvs_to_test:\n",
    "    if not csvs_to_test[key]:\n",
    "        continue\n",
    "    df = pd.read_csv(csvs_to_test[key])\n",
    "\n",
    "    for tup, test_sample in zip(df.itertuples(), test_json):\n",
    "        assert tup.claim == test_sample['claim']\n",
    "\n",
    "    predicted_labels[key] = df['verdict'].values\n",
    "    got_wrong[key] = df['verdict'].values != ground_truth\n",
    "    print(key, sum(got_wrong[key]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "409ea77d-a842-4895-9722-1cbcbad61222",
   "metadata": {},
   "source": [
    "# How many did every model get wrong?\n",
    "wrong_matrix = np.vstack([got_wrong[key] for key in got_wrong]) # shape #Claims x #Models\n",
    "trickiness_of_claims = wrong_matrix.sum(axis=0) / len(got_wrong) # percentage of models that got each claim wrong: shape #Claims"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60e6e201-2c6c-4e37-be4a-cd05d39ccf2e",
   "metadata": {},
   "source": [
    "trickiness_summary_stats = pd.DataFrame(trickiness_of_claims).describe()\n",
    "trickiness_summary_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f509c711-2091-4acf-9dfd-50132342bd64",
   "metadata": {},
   "source": [
    "## Get indexes of claims from each quartile\n",
    "\n",
    "# Get 1st, 2nd, 3rd, and 4th quartile claim indexes\n",
    "first_quartile = trickiness_of_claims <= trickiness_summary_stats[0]['25%']\n",
    "second_quartile = np.logical_and(trickiness_of_claims <= trickiness_summary_stats[0]['50%'], trickiness_of_claims > trickiness_summary_stats[0]['25%'])\n",
    "third_quartile = np.logical_and(trickiness_of_claims > trickiness_summary_stats[0]['50%'], trickiness_of_claims <= trickiness_summary_stats[0]['75%'])\n",
    "fourth_quartile = trickiness_of_claims > trickiness_summary_stats[0]['75%']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0166b850-d8ff-42b9-9e2c-18c29a49d060",
   "metadata": {},
   "source": [
    "# For each model, get the claims that it got wrong from each quartile\n",
    "incorrect_claims = {'ALL': {\n",
    "    '1st': first_quartile, \n",
    "    '2nd': second_quartile, \n",
    "    '3rd': third_quartile, \n",
    "    '4th': fourth_quartile},\n",
    "}\n",
    "\n",
    "\n",
    "for key in csvs_to_test:\n",
    "    if not csvs_to_test[key]:\n",
    "        continue\n",
    "    df = pd.read_csv(csvs_to_test[key])\n",
    "\n",
    "    for tup, test_sample in zip(df.itertuples(), test_json):\n",
    "        assert tup.claim == test_sample['claim']\n",
    "\n",
    "    wrong_predicted_labels = df['verdict'].values != ground_truth\n",
    "    test_claim_indexes = np.array(list(range(len(test_json))))\n",
    "\n",
    "    incorrect_claims[key] = {\n",
    "        '1st': test_claim_indexes[np.logical_and(wrong_predicted_labels, first_quartile)], \n",
    "        '2nd': test_claim_indexes[np.logical_and(wrong_predicted_labels, second_quartile)], \n",
    "        '3rd': test_claim_indexes[np.logical_and(wrong_predicted_labels, third_quartile)], \n",
    "        '4th': test_claim_indexes[np.logical_and(wrong_predicted_labels, fourth_quartile)]\n",
    "    }\n",
    "\n",
    "    incorrect_claims['ALL'] = {\n",
    "        '1st': np.logical_and(incorrect_claims['ALL']['1st'], wrong_predicted_labels), \n",
    "        '2nd': np.logical_and(incorrect_claims['ALL']['2nd'], wrong_predicted_labels), \n",
    "        '3rd': np.logical_and(incorrect_claims['ALL']['3rd'], wrong_predicted_labels), \n",
    "        '4th': np.logical_and(incorrect_claims['ALL']['4th'], wrong_predicted_labels)\n",
    "    }\n",
    "\n",
    "    print(key, incorrect_claims[key]['1st'].shape, incorrect_claims[key]['2nd'].shape, incorrect_claims[key]['3rd'].shape, incorrect_claims[key]['4th'].shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "868e686d-d9a5-4189-91b0-6ad910ed724a",
   "metadata": {},
   "source": [
    "# Get the number of claims that all models got incorrectly per trickiness level\n",
    "for key in incorrect_claims['ALL']:\n",
    "    print(key, incorrect_claims['ALL'][key].sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "566dff69-6acf-436d-879b-82ddb05052b6",
   "metadata": {},
   "source": [
    "# For each model, randomly sample a claim it got wrong from each quartile\n",
    "np.random.seed(42)\n",
    "\n",
    "sampled_incorrect_claims = dict()\n",
    "for key in incorrect_claims:\n",
    "    if key == 'ALL':\n",
    "        continue\n",
    "        \n",
    "    sampled_incorrect_claims[key] = dict()\n",
    "    for quartile_key in incorrect_claims[key]:\n",
    "        sampled_incorrect_claims[key][quartile_key] = dict()\n",
    "\n",
    "        if not incorrect_claims[key][quartile_key].any():\n",
    "            continue\n",
    "\n",
    "        random_incorrect_index = np.random.choice(incorrect_claims[key][quartile_key])\n",
    "        \n",
    "        sampled_incorrect_claims[key][quartile_key]['index'] = random_incorrect_index\n",
    "        sampled_incorrect_claims[key][quartile_key]['claim'] = test_json[random_incorrect_index]['claim']\n",
    "        sampled_incorrect_claims[key][quartile_key]['label'] = test_json[random_incorrect_index]['label']\n",
    "        sampled_incorrect_claims[key][quartile_key]['predicted'] = predicted_labels[key][random_incorrect_index]\n",
    "sampled_incorrect_claims['NumT5:Gold']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7af6d65f-eec2-4d7b-ad9d-78f43132f015",
   "metadata": {},
   "source": [
    "# For each model, randomly sample a claim it got wrong from each quartile\n",
    "np.random.seed(42)\n",
    "\n",
    "sampled_incorrect_claims = dict()\n",
    "for key in incorrect_claims:\n",
    "    if key == 'ALL':\n",
    "        continue\n",
    "        \n",
    "    sampled_incorrect_claims[key] = dict()\n",
    "    for quartile_key in incorrect_claims[key]:\n",
    "        sampled_incorrect_claims[key][quartile_key] = dict()\n",
    "\n",
    "        if not incorrect_claims[key][quartile_key].any():\n",
    "            continue\n",
    "\n",
    "        random_incorrect_index = np.random.choice(incorrect_claims[key][quartile_key])\n",
    "        \n",
    "        sampled_incorrect_claims[key][quartile_key]['index'] = random_incorrect_index\n",
    "        sampled_incorrect_claims[key][quartile_key]['claim'] = test_json[random_incorrect_index]['claim']\n",
    "        sampled_incorrect_claims[key][quartile_key]['label'] = test_json[random_incorrect_index]['label']\n",
    "        sampled_incorrect_claims[key][quartile_key]['predicted'] = predicted_labels[key][random_incorrect_index]\n",
    "sampled_incorrect_claims"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53b0595c-eca9-415e-af9d-3db4907305eb",
   "metadata": {},
   "source": [
    "# Randomly sample a claim that every model got wrong\n",
    "np.random.seed(42)\n",
    "\n",
    "all_incorrect = np.array(list(range(len(test_json))))[incorrect_claims['ALL']['4th']]\n",
    "\n",
    "sampled_all_incorrect_index = np.random.choice(all_incorrect)\n",
    "sampled_all_incorrect_claim = test_json[sampled_all_incorrect_index]['claim']\n",
    "sampled_all_incorrect_label = test_json[sampled_all_incorrect_index]['label']\n",
    "\n",
    "model_preds_for_all_incorrect = dict()\n",
    "for key in predicted_labels:\n",
    "    model_preds_for_all_incorrect[key] = predicted_labels[key][sampled_all_incorrect_index]\n",
    "\n",
    "sampled_all_incorrect_index, sampled_all_incorrect_claim, sampled_all_incorrect_label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "666419e5-165d-4d19-8059-8f90f2787195",
   "metadata": {},
   "source": [
    "model_preds_for_all_incorrect"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fea913f5-6a08-437b-bcba-dba934de6758",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
